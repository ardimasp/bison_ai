Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Index Keywords,Molecular Sequence Numbers,Chemicals/CAS,Manufacturers,Funding Details,Funding Text 1,Funding Text 2,References,Correspondence Address,Editors,Sponsors,Publisher,Conference name,Conference date,Conference location,Conference code,ISSN,ISBN,CODEN,PubMed ID,Language of Original Document,Abbreviated Source Title,Document Type,Publication Stage,Open Access,Source,EID
"Ivan A., Williem, Park I.K.","57202220044;51261491100;8612277600;","Joint Light Field Spatial and Angular Super-Resolution from a Single Image",2020,"IEEE Access","8",, 9119124,"112562","112573",,1,"10.1109/ACCESS.2020.3002921","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087612042&doi=10.1109%2fACCESS.2020.3002921&partnerID=40&md5=8f66a6c830e24cdd029bd333c419e445","Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea; Verihubs, South Jakarta, 12950, Indonesia","Ivan, A., Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea; Williem, Verihubs, South Jakarta, 12950, Indonesia; Park, I.K., Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea","Synthesizing a densely sampled light field from a single image is highly beneficial for many applications. Moreover, jointly solving both angular and spatial super-resolution problem also introduces new possibilities in light field imaging. The conventional method relies on physical-based rendering and a secondary network to solve the angular super-resolution problem. In addition, pixel-based loss limits the network capability to infer scene geometry globally. In this paper, we show that both super-resolution problems can be solved jointly from a single image by proposing a single end-to-end deep neural network that does not require a physical-based approach. Two novel loss functions based on known light field domain knowledge are proposed to enable the network to consider the relation between sub-aperture images. Experimental results show that the proposed model successfully synthesizes dense high resolution light field and it outperforms the state-of-the-art method in both quantitative and qualitative criteria. The method can be generalized to various scenes, rather than focusing on a particular subject. The synthesized light field can be used as if it has been captured by a light field camera, such as depth estimation and refocusing. © 2013 IEEE.","Deep neural network; light field; machine learning; super-resolution","Deep neural networks; Conventional methods; Depth Estimation; Domain knowledge; Network capability; Qualitative criteria; Secondary networks; State-of-the-art methods; Super resolution; Optical resolving power",,,,"Inha University, Inha: 2020-0-01389

Ministry of Science and ICT, South Korea, MSIT: 2017-0-00142

Institute for Information and Communications Technology Promotion, IITP

Samsung: SRFC-IT1702-06","This work was supported in part by the Samsung Research Funding Center of Samsung Electronics under Project SRFC-IT1702-06, in part by the Institute of Information and Communications Technology Planning and Evaluation (IITP) funded by the Korea Government (MSIT) (Development of Acceleration SW Platform Technology for On-device Intelligent Information Processing in Smart Devices) under Grant 2017-0-00142, and in part by the Artificial Intelligence Convergence Research Center (Inha University) under Grant 2020-0-01389.",,"https://raytrix.de/products/, Raytrix 3D Light Field Camera. Accessed: Aug. 9, 2018; Abadi, M., TensorFlow: A system for large-scale machine learning (2016) Proc. Osdi, 16, pp. 265-283; Shocher, A., Cohen, N., Irani, M., Zero-shot' super-resolution using deep internal learning (2018) Proc. Ieee Conf. Comput. Vis. Pattern Recog-nit., pp. 3118-3126. , Jun; Flynn, J., Neulander, I., Philbin, J., Snavely, N., Deep stereo: Learning to predict new views from the World's imagery (2016) Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 5515-5524. , Jun; Garg, R., Kumar, B.G.V., Carneiro, G., Reid, I., Unsupervised CNN for single view depth estimation: Geometry to the rescue (2016) Proc. Eur. Conf. Comput. Vis., pp. 740-756; Godard, C., Aodha, O.M., Brostow, G.J., Unsupervised monocular depth estimation with left-right consistency (2017) Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), 2, pp. 270-279. , Jul; Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K., Spatial transformer networks (2015) Proc. Conf. Neural Inf. Process. Syst., pp. 2017-2025; Johannsen, O., Sulc, A., Marniok, N., Goldluecke, B., Layered scene reconstruction from multiple light field camera views (2016) Proc. Asian Conf. Comput. Vis., pp. 3-18; Kalantari, N.K., Wang, T.-C., Ramamoorthi, R., Learning-based view synthesis for light field cameras (2016) Acm Trans. Graph., 35 (6), p. 193; Levoy, M., Hanrahan, P., Light field rendering (1996) Proc. 23rd Annu. Conf. Acm Trans. Graph., pp. 31-42; Maas, A.L., Hannun, A.Y., Ng, A.Y., Rectifier nonlinearities improve neural network acoustic models (2013) Proc. Int. Conf. Mach. Learn., 30, p. 3; Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A., Local light field fusion: Practical view synthesis with prescriptive sampling guidelines (2019) Acm Trans. Graph., 38 (4), pp. 1-14. , Jul; Ng, R., Levoy, M., Brédif, M., Duval, G., Horowitz, M., Hanrahan, P., Light field photography with a hand-held plenoptic camera (2005) Comput. Sci. Tech. Rep., 2 (11), pp. 1-11; Peng, J., Xiong, Z., Liu, D., Chen, X., Unsupervised depth estimation from light field using a convolutional neural network (2018) Proc. Int. Conf. 3D Vis. (3DV), pp. 295-303. , Sep; Schilling, H., Diebold, M., Rother, C., Jahne, B., Trust your model: Light field depth estimation with inline occlusion handling (2018) Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4530-4538. , Jun; Shin, C., Jeon, H.-G., Yoon, Y., Kweon, I.S., Kim, S.J., EPINET: A fully-convolutional neural network using epipolar geometry for depth from light field images (2018) Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4748-4757. , Jun; Srinivasan, P.P., Wang, T., Sreelal, A., Ramamoorthi, R., Ng, R., Learning to synthesize a 4D RGBD light field from a single image (2017) Proc. Ieee Int. Conf. Comput. Vis. (ICCV), pp. 2262-2270. , Oct; Srinivasan, P.P., Garg, R., Wadhwa, N., Ng, R., Barron, J.T., Aperture supervision for monocular depth estimation (2018) Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 6393-6401. , Jun; Srinivasan, P.P., Tucker, R., Barron, J.T., Ramamoorthi, R., Ng, R., Snavely, N., Pushing the boundaries of view extrapolation with multiplane images (2019) Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog-nit. (CVPR), pp. 175-184. , Jun; Tao, M.W., Hadap, S., Malik, J., Ramamoorthi, R., Depth from combining defocus and correspondence using light-field cameras (2013) Proc. Ieee Int. Conf. Comput. Vis., pp. 673-680. , Dec; Vianello, A., Ackermann, J., Diebold, M., Jahne, B., Robust Hough transform based 3D reconstruction from circular light fields (2018) Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 7327-7335. , Jun; Wang, Y., Liu, F., Wang, Z., Hou, G., Sun, Z., Tan, T., End-to-end view synthesis for light field imaging with pseudo 4DCNN (2018) Proc. Eur. Conf. Comput. Vis., pp. 340-355; Wanner, S., Goldluecke, B., Variational light field analysis for disparity estimation and super-resolution (2014) Ieee Trans. Pattern Anal. Mach. Intell., 36 (3), pp. 606-619. , Mar; Wilburn, B., Joshi, N., Vaish, V., Talvala, E.-V., Antunez, E., Barth, A., Adams, A., Levoy, M., High performance imaging using large camera arrays (2005) Acm Trans. Graph., 24 (3), pp. 765-776. , Jul; Williem, Park, I.K., Robust light field depth estimation for noisy scene with occlusion (2016) Proc. Ieee Conf. Comput. Vis. Pattern Recog-nit. (CVPR), pp. 4396-4404. , Jun; Williem, Park, I.K., Lee, K.M., Robust light field depth estimation using occlusion-noise aware data costs (2018) Ieee Trans. Pattern Anal. Mach. Intell., 40 (10), pp. 2484-2497. , Oct; Wu, G., Liu, Y., Fang, L., Dai, Q., Chai, T., Light field reconstruction using convolutional network on EPI and extended applications (2019) Ieee Trans. Pattern Anal. Mach. Intell., 41 (7), pp. 1681-1694. , Jul; Wu, G., Zhao, M., Wang, L., Dai, Q., Chai, T., Liu, Y., Light field reconstruction using deep convolutional network on EPI (2017) Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 6319-6327. , Jul; Xie, J., Girshick, R., Farhadi, A., Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks (2016) Proc. Eur. Conf. Comput. Vis., pp. 842-857; Yeung, H.W.F., Hou, J., Chen, J., Chung, Y.Y., Chen, X., Fast light field reconstruction with deep coarse-to-fine modeling of spatial-angular clues (2018) Proc. Eur. Conf. Comput. Vis., pp. 138-154; Zhang, Z., Liu, Y., Dai, Q., Light field from micro-baseline image pair (2015) Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3800-3809. , Jun; Zhou, T., Tulsiani, S., Sun, W., Malik, J., Efros, A.A., View synthesis by appearance flow (2016) Proc. Eur. Conf. Comput. Vis., pp. 286-301; Zhou, T., Tucker, R., Flynn, J., Fyffe, G., Snavely, N., Stereo magni-fication: Learning view synthesis using multiplane images (2018) Acm Trans. Graph., 37 (4), p. 65. , Aug; He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015) Proc. Ieee Int. Conf. Comput. Vis., pp. 1026-1034. , Dec; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015) Proc. Int. Conf. Learn. Represent., pp. 1-15","Park, I.K.; Department of Information and Communication Engineering, South Korea; email: pik@inha.ac.kr",,,"Institute of Electrical and Electronics Engineers Inc.",,,,,21693536,,,,"English","IEEE Access",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85087612042
"Lumentut J.S., Williem, Park I.K.","56563160800;51261491100;8612277600;","6-DOF motion blur synthesis and performance evaluation of light field deblurring",2019,"Multimedia Tools and Applications","78","23",,"33723","33746",,,"10.1007/s11042-019-08030-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071031511&doi=10.1007%2fs11042-019-08030-0&partnerID=40&md5=02f18379d627b47f8fcb415aa7e99bb8","Department Information and Communication Engineering, Inha University, Incheon, 22212, South Korea; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia","Lumentut, J.S., Department Information and Communication Engineering, Inha University, Incheon, 22212, South Korea; Williem, Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Park, I.K., Department Information and Communication Engineering, Inha University, Incheon, 22212, South Korea","Motion deblurring is essential for reconstructing sharp images from given a blurry input caused by the camera motion. The complexity of this problem increases in a light field due to its depth-dependent blur constraint. A method of generating synthetic 3 degree-of-freedom (3-DOF) translation blur on a light field image without camera rotation has been introduced. In this study, we generate a camera translation and rotation (6-DOF) motion blur model that preserves the consistency of the light field image. Our experiment results show that the proposed blur model can maintain the parallax information (depth-dependent blur) in a light field image. Furthermore, we produce a synthetic blurry light field dataset based on the 6-DOF model. Finally, to validate the usability of the synthetic dataset, we conduct extensive benchmarking using state-of-the-art motion deblurring algorithms. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","6-DOF; Deblur; Light field; Motion blur; Synthetic blur","Cameras; Degrees of freedom (mechanics); Geometrical optics; 6-DOF; Deblur; Light fields; Motion blur; Synthetic blur; Image enhancement",,,,"Inha University, Inha","This work was supported by Inha University Research Grant.",,"Bok, Y., Jeon, H.G., Kweon, I.S., Geometric calibration of micro-lense-based light field cameras using line features (2017) IEEE Trans Pattern Anal Mach Intell, 39 (2), pp. 287-300; Chandramouli, P., Jin, M., Perrone, D., Favaro, P., Plenoptic image motion deblurring (2018) IEEE Trans Image Process, 27 (4), pp. 1723-1734; Cho, D., Lee, M., Kim, S., Tai, Y.W., Modeling the calibration pipeline of the lytro camera for high quality light-field image reconstruction (2013) Proc of IEEE International Conference on Computer Vision, pp. 3280-3287; Cho, S., Lee, S., Fast motion deblurring (2009) ACM Trans Graph (TOG), 28 (5), p. 145; Dansereau, D.G., Eriksson, A., Leitner, J., Richardson-Lucy deblurring for moving light field cameras (2017) Proc of IEEE Conference on Computer Vision and Pattern Recognition Workshop, pp. 70-81; Dansereau, D.G., Pizarro, O., Williams, S.B., Decoding, calibration and rectification for lenselet-based plenoptic cameras (2013) Inl Proc of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1027-1034; Honauer, K., Johannsen, O., Kondermann, D., Goldluecke, B., A Dataset and Evaluation Methodology for Depth Estimation on 4D Light Fields (2017) Computer Vision – ACCV 2016, pp. 19-34. , Springer International Publishing, Cham; Jin, M., Chandramouli, P., Favaro, P., Bilayer blind deconvolution with the light field camera (2015) Proc of IEEE International Conference on Computer Vision Workshops, pp. 10-18; Joshi, N., Kang, S.B., Zitnick, C.L., Szeliski, R., Image deblurring using inertial measurement sensors (2010) ACM Trans Graph, 29 (4), p. 30; Kalantari, N.K., Wang, T.-C., Ramamoorthi, R., Learning-based view synthesis for light field cameras (2016) ACM Transactions on Graphics, 35 (6), pp. 1-10; Köhler, R., Hirsch, M., Mohler, B., Schölkopf, B., Harmeling, S., Recording and Playback of Camera Shake: Benchmarking Blind Deconvolution with a Real-World Database (2012) Computer Vision – ECCV 2012, pp. 27-40. , Springer Berlin Heidelberg, Berlin, Heidelberg; Krishnan, D., Tay, T., Fergus, R., Blind deconvolution using a normalized sparsity measure (2011) Proc of IEEE Conference on Computer Vision and Pattern Recognition; Lai, W.S., Huang, J.B., Hu, Z., Ahuja, N., Yang, M.H., A comparative study for single image blind deblurring (2016) Proc of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1701-1709; Lee, D., Park, H., Park, I.K., Lee, K.M., Joint blind motion deblurring and depth estimation of light field (2018) Proc of European Conference on Computer Vision; Levin, A., Fergus, R., Durand, F., Freeman, W.T., Image and depth from a conventional camera with a coded aperture (2007) ACM Trans Graph, 26 (3), p. 70; Light Field Toolbox for Matlab V0.4, , https://bit.ly/2JptsOT; http://www.lytro.com/, The Lytro camera; Mahesh Mohan, M., Rajagopalan, A., Divide and conquer for full-resolution light field deblurring (2018) Proc of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6421-6429; Pan, J., Hu, Z., Su, Z., Yang, M.H., Deblurring text images via L 0 -regularized intensity and gradient prior (2014) Proc of IEEE Conference on Computer Vision and Pattern Recognition, pp. 2901-2908; Pan, J., Sun, D., Pfister, H., Yang, M.H., Blind image deblurring using dark channel prior (2016) Proc of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1628-1636; Sellent, A., Rother, C., Roth, S., Stereo Video Deblurring (2016) Computer Vision – ECCV 2016, pp. 558-575. , Springer International Publishing, Cham; Srinivasan, P.P., Ng, R., Ramamoorthi, R., Light field blind motion deblurring (2017) Proc of IEEE Conference on Computer Vision and Pattern Recognition, pp. 3958-3966; Srinivasan, P.P., Wang, T., Sreelal, A., Ng, R., Learning to synthesize a 4D RGBD light field from a single image (2017) Proc of IEEE International Conference on Computer Vision, pp. 2262-2270; Tai, Y.W., Tan, P., Brown, M.S., Richardson-lucy deblurring for scenes under a projective motion path (2011) IEEE Trans Pattern Anal Mach Intell, 33 (8), pp. 1603-1618; The (New) Stanford Light Field Archive, , http://lightfield.stanford.edu/lfs.html; Wang, T.C., Efros, A., Ramamoorthi, R., Depth estimation with occlusion modeling using light-field cameras (2016) IEEE Trans Pattern Anal Mach Intell, 38 (11), pp. 2170-2181; Whyte, O., Sivic, J., Zisserman, A., Ponce, J., Non-uniform deblurring for shaken images (2010) Proc of IEEE Conference on Computer Vision and Pattern Recognition, 2, pp. 168-186; Whyte, O., Sivic, J., Zisserman, A., Ponce, J., Non-uniform deblurring for shaken images (2012) Int J Comput Vis, 98, pp. 157-170; Williem, W., Park, I.K., Lee, K.M., Robust light field depth estimation using occlusion-noise aware data costs (2018) IEEE Trans Pattern Anal Mach Intell, 40 (10), pp. 2484-2497; Xu, L., Jia, J., Two-Phase Kernel Estimation for Robust Motion Deblurring (2010) Computer Vision – ECCV 2010, pp. 157-170. , Springer Berlin Heidelberg, Berlin, Heidelberg; Xu, L., Jia, J., Depth-aware motion deblurring (2012) Proc of IEEE International Conference on Computational Photography, pp. 1-8; Xu, L., Zheng, S., Jia, J., Unnatural L 0 sparse representation for natural image deblurring (2013) Proc of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1107-1114; Yan, C., Li, L., Zhang, C., Liu, B., Zhang, Y., Dai, Q., (2019) Cross-Modality Bridging and Knowledge Transferring for Image Understanding, , IEEE Trans on Multimedia","Park, I.K.; Department Information and Communication Engineering, South Korea; email: pik@inha.ac.kr",,,"Springer",,,,,13807501,,MTAPF,,"English","Multimedia Tools Appl",Article,"Final","",Scopus,2-s2.0-85071031511
"Ivan A., Williem, Park I.K.","57202220044;51261491100;8612277600;","Light field depth estimation on off-the-shelf mobile GPU",2018,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June",, 8575256,"747","756",,1,"10.1109/CVPRW.2018.00106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060876834&doi=10.1109%2fCVPRW.2018.00106&partnerID=40&md5=a37e15047c699ee63f07ca730193b711","Dept. of Information and Communication Eng., Inha University, Incheon, 22212, South Korea; School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia","Ivan, A., Dept. of Information and Communication Eng., Inha University, Incheon, 22212, South Korea; Williem, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Park, I.K., Dept. of Information and Communication Eng., Inha University, Incheon, 22212, South Korea","While novel light processing algorithms have been continuously introduced, it is still challenging to perform light field processing on a mobile device with limited computation resource due to the high dimensionality of light field data. Recently, the performance of mobile graphics processing unit (GPU) increases rapidly and GPGPU on mobile GPU utilizes massive parallel computation to solve various computer vision problems with high computational complexity. To show the potential capability of light field processing on mobile GPU, we parallelize and optimize the state-of-the-art light field depth estimation which is essential to many light field applications. We employ both algorithm and kernel-based optimization to enable light field processing on mobile GPU. Light field processing involves independent pixel processing with intensive floating-point operations that can be vectorized to match single instruction multiple data (SIMD) style of GPU architecture. We design efficient memory access, caching, and prefetching to exploit light field properties. The experimental result shows that the light field depth estimation on mobile GPU obtains comparable performance as on the desktop CPU. The proposed optimization method gains up to 25 times speedup compared to the naïve baseline method. © 2018 IEEE.",,"Computer graphics; Computer graphics equipment; Computer vision; Digital arithmetic; Memory architecture; Program processors; Computation resources; Computer vision problems; Floating point operations; High dimensionality; Optimization method; Parallel Computation; Potential capability; Single instruction multiple data; Graphics processing unit",,,,"Ministry of Science and ICT, South Korea, MSIT: 2017-0-00142

Institute for Information and Communications Technology Promotion, IITP

Institute for Information and Communications Technology Promotion, IITP","This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korean government (MSIT) (2017-0-00142).",,"Lytro 3D Light Field Camera., , https://www.lytro.com/; OpenCL Best Practices., , https://www.nvidia.com/content/cudazone/CUDABrowser/downloads/papers/NVIDIA_OpenCL_BestPracticesGuide.pdf; Raytrix 3D Light Field Camera., , https://raytrix.de/products/; Agus, M., Gobbetti, E., Guitin, J.A.I., Marton, F., Pintore, G., GPU accelerated direct volume rendering on an interactive light field display (2008) Computer Graphics Forum, 27 (2), pp. 231-240. , April; Barron, J.T., Adams, A., Shih, Y., Hernndez, C., Fast bilateral-space stereo for synthetic defocus (2015) IEEE Conference on Computer Vision and Pattern Recognition, pp. 4466-4474. , June; Chang, C.W., Chen, M.R., Hsu, P.H., Lu, Y.C., A pixel-based depth estimation algorithm and its hardware implementation for 4-D light field data (2014) IEEE International Symposium on Circuits and Systems, pp. 786-789. , June; Che, S., Li, J., Sheaffer, J.W., Skadron, K., Lach, J., Accelerating compute-intensive applications with GPUs and FPGAS (2008) Symposium on Application Specific Processors, pp. 101-107. , June; Chen, C.C., Chiang, S.C.F., Huang, X.X., Su, M.S., Lu, Y.C., Depth estimation of light field data from pinholemasked DSLR cameras (2010) IEEE International Conference on Image Processing, pp. 1769-1772. , September; Choi, Y.K., Park, I.K., Efficient GPU-based graph cuts for stereo matching (2013) IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 642-648. , June; Dansereau, D.G., Pizarro, O., Williams, S.B., Decoding, calibration and rectification for lenselet-based plenoptic cameras (2013) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1027-1034. , June; Eibensteiner, F., Kogler, J., Scharinger, J., A highperformance hardware architecture for a frameless stereo vision algorithm implemented on a FPGA platform (2014) IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 637-644. , June; Fiss, J., Curless, B., Szeliski, R., Light field layer matting (2015) IEEE Conference on Computer Vision and Pattern Recognition, pp. 623-631. , June; Heber, S., Pock, T., Convolutional networks for shape from light field (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 3746-3754. , June; Hofmann, J., Korinth, J., Koch, A., A scalable highperformance hardware architecture for real-time stereo vision by semi-global matching (2016) IEEE Conference on Computer Vision and Pattern RecognitionWorkshops, pp. 845-853. , June; Hou, X., Wei, L.-Y., Shum, H.-Y., Guo, B., Real-time multi-perspective rendering on graphics hardware (2006) Eurographics Conference on Rendering Techniques, pp. 93-102. , June; Jarabo, A., Masia, B., Bousseau, A., Pellacini, F., Gutierrez, D., How do people edit light fields? (2014) ACM Trans. on Graphics, 33 (4), p. 146. , July; Jones, A., McDowall, I., Yamada, H., Bolas, M., Debevec, P., Rendering for an interactive 360? Light field display (2007) ACM Trans. on Graphics, , August; Lanman, D., Luebke, D., Near-eye light field displays (2013) ACM Trans. on Graphics, p. 111. , November; Li, J., Lu, M., Li, Z.N., Continuous depth map reconstruction from light fields (2015) IEEE Trans. on Image Processing, 24 (11), pp. 3257-3265. , July; Li, N., Ye, J., Ji, Y., Ling, H., Yu, J., Saliency detection on light field (2017) IEEE Trans. on Pattern Analysis and Machine Intelligence, 39 (8), pp. 1605-1616. , August; Ng, R., Fourier slice photography (2005) ACM Trans. on Graphics, 24 (3), pp. 735-744. , July; Park, I.K., Singhal, N., Lee, M.H., Cho, S., Kim, C., Design and performance evaluation of image processing algorithms on GPUS (2011) IEEE Trans. on Parallel and Distributed Systems, 22 (1), pp. 91-104. , June; Srinivasan, P.P., Ng, R., Ramamoorthi, R., Light field blind motion deblurring (2017) IEEE Conference on Computer Vision and Pattern Recognition, pp. 3958-3966. , July; Stone, J.E., Gohara, D., Shi, G., OpenCL: A parallel programming standard for heterogeneous computing systems (2010) Computing in Science Engineering, 12 (3), pp. 66-73. , May; Tao, M.W., Hadap, S., Malik, J., Ramamoorthi, R., Depth from combining defocus and correspondence using lightfield cameras (2013) IEEE International Conference on Computer Vision, pp. 673-680. , December; Tao, M.W., Srinivasan, P.P., Malik, J., Rusinkiewicz, S., Ramamoorthi, R., Depth from shading, defocus, and correspondence using light-field angular coherence (2015) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1940-1948. , June; Vaish, V., Levoy, M., Szeliski, R., Zitnick, C.L., Kang, S.B., Reconstructing occluded surfaces using synthetic apertures: Stereo, focus and robust measures (2006) IEEE Conference on Computer Vision and Pattern Recognition, 2, pp. 2331-2338. , February; Wang, T.C., Efros, A.A., Ramamoorthi, R., Depth estimation with occlusion modeling using light-field cameras (2016) IEEE Trans. on Pattern Analysis and Machine Intelligence, 38 (11), pp. 2170-2181. , January; Wang, T.-C., Zhu, J.-Y., Hiroaki, E., Chandraker, M., Efros, A., Ramamoorthi, R., A 4D light-field dataset and CNN architectures for material recognition (2016) European Conference on Computer Vision, pp. 121-138. , February; Wanner, S., Goldluecke, B., Globally consistent depth labelling of 4D lightfields (2012) IEEE Conference on Computer Vision and Pattern Recognition, pp. 41-48. , June; Williem, Park, I.K., Robust light field depth estimation for noisy scene with occlusion (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 4396-4404. , June; Williem, Park, I.K., Lee, K.M., Robust light field depth estimation using occlusion-noise aware data costs IEEE Trans. on Pattern Analysis and Machine Intelligence, , in press; Williem, Won, S.K., Park, I.K., Spatio-angular consistent editing framework for 4D light field images (2016) Multimedia Tools and Applications, 75 (23), pp. 16615-16631. , December; Wu, G., Masia, B., Jarabo, A., Zhang, Y., Wang, L., Dai, Q., Chai, T., Liu, Y., Light field image processing: An overview (2017) IEEE Journal of Selected Topics in Signal Processing, 11 (7), pp. 926-954. , October; Yu, Z., Guo, X., Ling, H., Lumsdaine, A., Yu, J., Line assisted light field triangulation and stereo matching (2013) IEEE International Conference on Computer Vision, pp. 2792-2799. , June; Yuttakonkit, Y., Nakashima, Y., Performance comparison of CGRA and mobile GPU for light-field image processing (2016) Fourth International Symposium on Computing and Networking, pp. 174-180. , November; Zhang, S., Sheng, H., Li, C., Zhang, J., Xiong, Z., Robust depth estimation for light field via spinning parallelogram (2016) Computer Vision and Image Understanding, 145, pp. 148-159. , April",,,,"IEEE Computer Society","31st Meeting of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPRW 2018","18 June 2018 through 22 June 2018",,143792,21607508,9781538661000,,,"English","IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops",Conference Paper,"Final","",Scopus,2-s2.0-85060876834
"Williem, Park I.K.","51261491100;8612277600;","Cost aggregation benchmark for light field depth estimation",2018,"Journal of Visual Communication and Image Representation","56",,,"38","51",,2,"10.1016/j.jvcir.2018.08.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052662881&doi=10.1016%2fj.jvcir.2018.08.015&partnerID=40&md5=f14a1a23a4cf832fd02c08fcb9655d4d","Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea","Williem, Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Park, I.K., Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea","Light field depth estimation has become a mature research topic and there are numerous algorithms introduced by various research groups. However, comprehensive and fair benchmark is difficult to apply because there are large step variances of the introduced algorithms. It is essential to analyze each step in the light field depth estimation so that it could help design better and more robust algorithms. Thus, a thorough analysis of cost aggregation is conducted in this paper to analyze the performance of various cost aggregation methods on light field depth estimation. A study on the parameter setting for each cost aggregation method is performed. Then, each cost aggregation with its optimal parameters is evaluated individually. Instead of using the standard rank system, this paper utilizes the weighted rank system based on the score difference on each criterion. Experimental results confirm that the guided-filter based method outperforms other methods in most evaluation criteria. © 2018 Elsevier Inc.","Benchmark; Cost aggregation; Depth estimation; Light field; Weighted rank","Benchmarking; Cost estimating; Cost aggregations; Depth Estimation; Evaluation criteria; Light fields; Optimal parameter; Parameter setting; Robust algorithm; Weighted rank; Cost benefit analysis",,,,"Inha University, Inha","This work was supported by Inha University Research Grant.",,"Williem, R., Raskar, I.K., Park, Depth map estimation and colorization of anaglyph images using local color prior and reverse intensity distribution (2015) Proc. of IEEE International Conference on Computer Vision, pp. 3460-3468. , IEEE Computer Society; Yan, C., Xie, H., Yang, D., Yin, J., Zhang, Y., Dai, Q., Supervised hash coding with deep neural network for environment perception of intelligent vehicles (2018) IEEE Trans. Intell. Transportation Syst., 19 (1), pp. 284-295; Yan, C., Xie, H., Liu, S., Yin, J., Zhang, Y., Dai, Q., Effective uyghur language text detection in complex background images for traffic prompt identification (2018) IEEE Trans. Intell. Transportation Syst., 19 (1), pp. 220-229; Yan, C., Xie, H., Chen, J., Zha, Z.-J., Hao, X., Zhang, Y., Dai, Q., https://doi.org/10.1109/TMM.2018.2838320, An effective uyghur text detector for complex background images, IEEE Trans. Multimedia doi; Srinivasan, P.P., Ng, R., Ramamoorthi, R., Light field blind motion deblurring (2017) Proc. of IEEE International Conference on Computer Vision and Pattern Recognition, pp. 3958-3966. , IEEE Computer Society; Yoon, Y., Jeon, H.-G., Yoo, D., Lee, J.-Y., Kweon, I.S., Light-field image super-resolution using convolutional neural network (2017) IEEE Signal Process. Lett., 24 (6), pp. 848-852; https://doi.org/10.1109/TPAMI.2017.2746858; Jarabo, A., Masia, B., Bousseau, A., Pellacini, F., Gutierrez, D., How do people edit light fields? (2014) ACM Trans. Graphics (TOG), 33 (4), p. 146; Williem, Won, S.K., Park, I.K., Spatio-angular consistent editing framework for 4d light field images (2016) Multimedia Tools Appl., 75 (23), pp. 16615-16631; Wang, T.-C., Zhu, J.-Y., Kalantari, N.K., Efros, A.A., Ramamoorthi, R., Light field video capture using a learning-based hybrid imaging system (2017) ACM Trans. Graphics, 36 (4), pp. 1331-133:13; Johannsen, O., Honauer, K., Goldluecke, B., Alperovich, A., Battisti, F., Bok, Y., Brizzi, M., Zhu, H., A taxonomy and evaluation of dense light field depth estimation algorithms (2017) Proc. of IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 82-99. , IEEE Computer Society; Hosni, A., Rhemann, C., Bleyer, M., Rother, C., Gelautz, M., Fast cost-volume filtering for visual correspondence and beyond (2013) IEEE Trans. Pattern Anal. Machine Intelligence, 35 (2), pp. 504-511; Yang, Q., A non-local cost aggregation method for stereo matching (2012) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1402-1409. , IEEE Computer Society; Mei, X., Sun, X., Dong, W., Wang, H., Zhang, X., Segment-tree based cost aggregation for stereo matching (2013) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 313-320. , IEEE Computer Society; Pham, C.C., Jeon, J.W., Domain transformation-based efficient cost aggregation for local stereo matching (2013) IEEE Trans. Circuits Syst. Video Technol., 23 (7), pp. 1119-1130; Wanner, S., Meister, S., Goldluecke, B., Datasets and benchmarks for densely sampled 4D light fields (2013) Proc. of Vision Modeling & Visualization, pp. 225-226. , Eurographics Association; Honauer, K., Johannsen, O., Kondermann, D., Goldluecke, B., A dataset and evaluation methodology for depth estimation on 4D light fields (2016) Proc. of Asian Conference on Computer Vision, pp. 19-34. , Springer; Chen, C., Lin, H., Yu, Z., Kang, S.B., Yu, J., Light field stereo matching using bilateral statistics of surface cameras (2014) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1518-1525. , IEEE Computer Society; He, K., Sun, J., Tang, X., Guided image filtering (2013) IEEE Trans. Pattern Anal. Machine Intelligence, 35 (6), pp. 1397-1409; Jeon, H.G., Park, J., Choe, G., Park, J., Bok, Y., Tai, Y.W., Kweon, I.S., Accurate depth map estimation from a lenslet light field camera (2015) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1547-1555. , IEEE Computer Society; Boykov, Y., Veksler, O., Zabih, R., Fast approximate energy minimization via graph cuts (2001) IEEE Trans. Pattern Anal. Machine Intelligence, 23 (11), pp. 1222-1239; Ma, Z., He, K., Wei, Y., Sun, J., Wu, E., Constant time weighted median filtering for stereo matching and beyond (2013) Proc. of IEEE International Conference on Computer Vision, pp. 49-56. , IEEE Computer Society; Yang, Q., Yang, R., Davis, J., Nister, D., Spatial-depth super resolution for range images (2007) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-8. , IEEE Computer Society; Zhang, S., Sheng, H., Li, C., Zhang, J., Xiong, Z., Robust depth estimation for light field via spinning parallelogram (2016) Comput. Vis. Image Underst., 145, pp. 148-159; Sheng, H., Zhao, P., Zhang, S., Zhang, J., Yang, D., Occlusion-aware depth estimation for light field using multi-orientation epis (2018) Pattern Recogn., 74, pp. 587-599; Williem, I.K., Park, Robust light field depth estimation for noisy scene with occlusion (2016) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 4396-4404. , IEEE Computer Society; Wang, T.C., Efros, A.A., Ramamoorthi, R., Depth estimation with occlusion modeling using light-field cameras (2016) IEEE Trans. Pattern Anal. Machine Intelligence, 38 (11), pp. 2170-2181; Heber, S., Pock, T., Shape from light field meets robust pca (2014) Proc. of European Conference on Computer Vision, pp. 751-767. , Springer; Tao, M.W., Hadap, S., Malik, J., Ramamoorthi, R., Depth from combining defocus and correspondence using light-field cameras (2013) Proc. of IEEE International Conference on Computer Vision, pp. 673-680. , IEEE Computer Society; Tao, M.W., Srinivasan, P.P., Malik, J., Rusinkiewicz, S., Ramamoorthi, R., Depth from shading, defocus, and correspondence using light-field angular coherence (2015) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1940-1948. , IEEE Computer Society; Wanner, S., Goldluecke, B., Globally consistent depth labelling of 4D lightfields (2012) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 41-48. , IEEE Computer Society; Hosni, A., Bleyer, M., Gelautz, M., Secrets of adaptive support weight techniques for local stereo matching (2013) Comput. Vis. Image Underst., 117 (6), pp. 620-632; Gastal, E.S.L., Oliveira, M.M., Domain transform for edge-aware image and video processing (2011) ACM Trans. Graphics (TOG), 30 (4), p. 69; Scharstein, D., Szeliski, R., A taxonomy and evaluation of dense two-frame stereo correspondence algorithms (2002) Int. J. Comput. Vision, 47 (1), pp. 7-42","Park, I.K.; Department of Information and Communication Engineering, South Korea; email: pik@inha.ac.kr",,,"Academic Press Inc.",,,,,10473203,,JVCRE,,"English","J Visual Commun Image Represent",Article,"Final","",Scopus,2-s2.0-85052662881
"Williem, Park I.K., Lee K.M.","51261491100;8612277600;26642943400;","Robust Light Field Depth Estimation Using Occlusion-Noise Aware Data Costs",2018,"IEEE Transactions on Pattern Analysis and Machine Intelligence","40","10", 8022875,"2484","2497",,35,"10.1109/TPAMI.2017.2746858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028701128&doi=10.1109%2fTPAMI.2017.2746858&partnerID=40&md5=ae08b3147e611450d52a3325a2a2695d","Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, 08826, South Korea","Williem, Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Park, I.K., Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea; Lee, K.M., Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, 08826, South Korea","Depth estimation is essential in many light field applications. Numerous algorithms have been developed using a range of light field properties. However, conventional data costs fail when handling noisy scenes in which occlusion is present. To address this problem, we introduce a light field depth estimation method that is more robust against occlusion and less sensitive to noise. Two novel data costs are proposed, which are measured using the angular patch and refocus image, respectively. The constrained angular entropy cost (CAE) reduces the effects of the dominant occluder and noise in the angular patch, resulting in a low cost. The constrained adaptive defocus cost (CAD) provides a low cost in the occlusion region, while also maintaining robustness against noise. Integrating the two data costs is shown to significantly improve the occlusion and noise invariant capability. Cost volume filtering and graph cut optimization are applied to improve the accuracy of the depth map. Our experimental results confirm the robustness of the proposed method and demonstrate its ability to produce high-quality depth maps from a range of scenes. The proposed method outperforms other state-of-the-art light field depth estimation methods in both qualitative and quantitative evaluations. © 2018 IEEE.","constrained adaptive defocus; constrained angular entropy; data cost; depth estimation; Light field; noise-aware; occlusion-aware","Cameras; Constrained optimization; Cost estimating; Cost reduction; Entropy; Estimation; Graphic methods; Robustness (control systems); Data costs; Defocus; Depth Estimation; Image color analysis; Light fields; noise-aware; occlusion-aware; Optimization method; Cost benefit analysis",,,,"Ministry of Science and ICT, South Korea, MSIT

National Research Foundation of Korea, NRF","This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2016R1A2B4014731).",,"(2014) The Lytro Camera, , https://www.lytro.com, [Online]; (2013) Raytrix, 3D Light Field Camera Technology, , www.raytrix.de, [Online]; Wilburn, B., High performance imaging using large camera arrays (2005) ACM Trans. Graph., 24 (3), pp. 765-779; Ng, R., Fourier slice photography (2005) ACM Trans. Graph., 24 (3), pp. 735-744; Li, N., Ye, J., Ji, Y., Ling, H., Yu, J., Saliency detection on light field (2014) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 2806-2813; Cho, D., Kim, S., Tai, Y.-W., Consistent matting for light field images (2014) Proc. Eur. Conf. Comput. Vis., pp. 90-104; Jarabo, A., Masia, B., Bousseau, A., Pellacini, F., Gutierrez, D., How do people edit light fields? (2014) ACM Trans. Graph., 33 (4); Chen, C., Lin, H., Yu, Z., Kang, S.B., Yu, J., Light field stereo matching using bilateral statistics of surface cameras (2014) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1518-1525; Lin, H., Chen, C., Kang, S.B., Yu, J., Depth recovery from light field using focal stack symmetry (2015) Proc. IEEE Int. Conf. Comput. Vis., pp. 3451-3459; Heber, S., Ranftl, R., Pock, T., Variational shape from light field (2013) Proc. Int. Conf. Energy Minimization Methods Comput. Vis. Pattern Recognit, pp. 66-79; Heber, S., Pock, T., Shape from light field meets robust PCA (2014) Proc. Eur. Conf. Comput. Vis., pp. 751-767; Jeon, H.G., Accurate depth map estimation from a lenslet light field camera (2015) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1547-1555; Matoušek, M., Werner, T., Hlavác, V., Accurate correspondences from epipolar plane images (2001) Proc. Comput. Vis. Winter Workshop, pp. 181-189; Tao, M.W., Hadap, S., Malik, J., Ramamoorthi, R., Depth from combining defocus and correspondence using light-field cameras (2013) Proc. IEEE Int. Conf. Comput. Vis., pp. 673-680; Tao, M.W., Srinivasan, P.P., Malik, J., Rusinkiewicz, S., Ramamoorthi, R., Depth from shading, defocus, and correspondence using light-field angular coherence (2015) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1940-1948; Tao, M.W., Wang, T.C., Malik, J., Ramamoorthi, R., Depth estimation for glossy surfaces with light-field cameras (2014) Proc. Eur. Conf. Comput. Vis. Workshops, pp. 533-547; Tošić, I., Berkner, K., Light field scale-depth space transform for dense depth estimation (2014) Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, pp. 441-448; Vaish, V., Levoy, M., Szeliski, R., Zitnick, C.L., Kang, S.B., Reconstructing occluded surfaces using synthetic apertures: Stereo, focus and robust measures (2006) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 2331-2338; Wang, T.C., Efros, A.A., Ramamoorthi, R., Occlusion-aware depth estimation using light-field cameras (2015) Proc. IEEE Int. Conf. Comput. Vis., pp. 3487-3495; Wanner, S., Goldluecke, B., Globally consistent depth labelling of 4D lightfields (2012) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 41-48; Williem, Park, I.K., Robust light field depth estimation for noisy scene with occlusion (2016) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 4396-4404; Zhang, S., Sheng, H., Li, C., Zhang, J., Xiong, Z., Robust depth estimation for light field via spinning parallelogram (2016) Comput. Vis. Image Understanding, 145, pp. 148-159; Yu, Z., Guo, X., Ling, H., Lumsdaine, A., Yu, J., Line assisted light field triangulation and stereo matching (2013) Proc. IEEE Int. Conf. Comput. Vis., pp. 2792-2799; Wanner, S., Meister, S., Goldluecke, B., Datasets and benchmarks for densely sampled 4D light fields (2013) Proc. Vis. Model. Vis., pp. 225-226; Bolles, R., Baker, H., Marimont, D., Epipolar-plane image analysis: An approach to determining structure from motion (1987) Int. J. Comput. Vis., 1 (1), pp. 7-55; Criminisi, A., Kang, S.B., Swaminathan, R., Szeliski, R., Anandan, P., Extracting layers and analyzing their specular properties using epipolar-plane-image analysis (2005) Comput. Vis. Image Understanding, 97 (1), pp. 51-85; Kim, C., Zimmer, H., Pritch, Y., Sorkine-Hornung, A., Gross, M., Scene reconstruction from high spatio-angular resolution light fields (2013) ACM Trans. Graph., 32 (4); Kolmogorov, V., Zabih, R., Multi-camera scene reconstruction via graph cuts (2002) Proc. Eur. Conf. Comput. Vis., pp. 82-96; Wei, Y., Quan, L., Asymmetrical occlusion handling using graph cut for multi-view stereo (2005) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 902-909; Bleyer, M., Rother, C., Kohli, P., Surface stereo with soft segmentation (2010) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1570-1577; Kang, S.B., Szeliski, R., Chai, J., Handling occlusions in dense multi-view stereo (2001) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 103-110; Boykov, Y., Veksler, O., Zabih, R., Fast approximate energy minimization via graph cuts (2001) IEEE Trans. Pattern Anal. and Mach. Intell., 23 (11), pp. 1222-1239. , Nov; He, K., Sun, J., Tang, X., Guided image filtering (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (6), pp. 1397-1409. , Jun; Hosni, A., Rhemann, C., Bleyer, M., Rother, C., Gelautz, M., Fast cost-volume filtering for visual correspondence and beyond (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (2), pp. 504-511. , Feb; Dansereau, D.G., Pizarro, O., Williams, S.B., Decoding, calibration and rectification for lenselet-based plenoptic cameras (2013) Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1027-1034","Park, I.K.; Department of Information and Communication Engineering, South Korea; email: pik@inha.ac.kr",,,"IEEE Computer Society",,,,,01628828,,ITPID,"28866482","English","IEEE Trans Pattern Anal Mach Intell",Article,"Final","",Scopus,2-s2.0-85028701128
"Williem, Park I.K.","51261491100;8612277600;","Deep self-guided cost aggregation for stereo matching",2018,"Pattern Recognition Letters","112",,,"168","175",,10,"10.1016/j.patrec.2018.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049779130&doi=10.1016%2fj.patrec.2018.07.010&partnerID=40&md5=7ee29608dfccdcc24bafd9663d05022c","Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea","Williem, Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Park, I.K., Department of Information and Communication Engineering, Inha University, Incheon, 22212, South Korea","In this paper, we present a deep self-guided cost aggregation method used to obtain an accurate disparity map from a pair of stereo images. Conventional cost aggregation methods typically perform joint image filtering on each cost volume slice. Thus, a guidance image is necessary for the conventional methods to work effectively. However, a guidance image might be unreliable due to several distortions, such as noise, blur, radiometric variation. Based on our observations, each cost volume slice can guide itself based on the internal features. However, finding a direct mapping function from the initial and filtered cost volume slice without any guidance image is difficult. To solve this problem, we use an advanced deep learning technique to perform self-guided cost aggregation. Because of the absence of ground truth cost volume, we offer the solution for the dataset generation. Our proposed deep learning network consists of two sub-networks: dynamic weight network and descending filtering network. We integrate the feature reconstruction loss and the pixelwise mean square loss function to preserve the edge property. Experimental results show that the proposed method achieves better results even though it does not employ a guidance image. © 2018 Elsevier B.V.","cost aggregation; deep learning; guided-filter; stereo matching","Costs; Deep learning; Conventional methods; Cost aggregations; Feature reconstruction; Guided filters; Internal features; Learning techniques; Radiometric variations; Stereo matching; Stereo image processing",,,,"Ministry of Science, ICT and Future Planning, MSIP

National Research Foundation of Korea, NRF","This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. NRF-2016R1A2B4014731 ).",,"Brabandere, B.D., Jia, X., Tuytelaars, T., Gool, L.V., Dynamic filter networks (2016) Proc. of Conference on Neural Information and Processing Systems, pp. 667-675; Chen, C.-H., Zhou, H., Ahonen, T., Blur-aware disparity estimation from defocus stereo images (2015) Proc. of IEEE International Conference on Computer Vision, pp. 855-863; Choi, O., Chang, H.S., Yet another cost aggregation over models (2016) IEEE Trans. Image Process., 25 (11), pp. 5397-5410; Gastal, E.S.L., Oliveira, M.M., Domain transform for edge-aware image and video processing (2011) ACM Trans. Graphics (TOG), 30 (4), p. 69; Geiger, A., Efficient large-scale stereo matching (2010) Proc. of Asian Conference on Computer Vision, pp. 25-38; He, K., Sun, J., Tang, X., Guided image filtering (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (6), pp. 1397-1409; Heo, Y.S., Lee, K.M., Lee, S.U., Simultaneous depth reconstruction and restoration of noisy stereo images using non-local pixel distribution (2007) Proc. of IEEE Conference on Computer Vision and Pattern Recognition; Heo, Y.S., Lee, K.M., Lee, S.U., Robust stereo matching using adaptive normalized cross correlation (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33 (4), pp. 807-822; Hirschmüller, H., Scharstein, D., Evaluation of stereo matching costs on images with radiometric differences (2009) IEEE Trans. Pattern Anal. Mach. Intell., 31 (9), pp. 1582-1599; Hong, L., Chen, G., Segment-based stereo matching using graph cuts (2004) Proc. of IEEE Conference on Computer Vision and Pattern Recognition; Hosni, A., Bleyer, M., Gelautz, M., Secrets of adaptive support weight techniques for local stereo matching (2013) Comput. Vision Image Understand., 117 (6), pp. 620-632; Hosni, A., Rhemann, C., Bleyer, M., Rother, C., Gelautz, M., Fast cost-volume filtering for visual correspondence and beyond (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (2), pp. 504-511; Jeon, H.-G., Lee, J.-Y., Im, S., Ha, H., Kweon, I.S., Stereo matching with color and monochrome cameras in low-light conditions (2016) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 4086-4094; Johnson, J., Alahi, A., Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution (2016) Proc. of European Conference on Computer Vision, pp. 694-711; Mei, X., Sun, X., Dong, W., Wang, H., Zhang, X., Segment-tree based cost aggregation for stereo matching (2013) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 313-320; Menze, M., Geiger, A., Object scene flow for autonomous vehicles (2015) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 3061-3070; Menze, M., Heipke, C., Geiger, A., Joint 3D estimation of vehicles and scene flow (2015) Proc. of ISPRS Workshop on Image Sequence Analysis; Min, D., Lu, J., Do, M.N., Joint histogram-based cost aggregation for stereo matching (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (10), pp. 2539-2545; Park, M.-G., Yoon, K.-J., Leveraging stereo matching with learning-based confidence measures (2015) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 101-109; Pham, C.C., Jeon, J.W., Domain transformation-based efficient cost aggregation for local stereo matching (2013) IEEE Trans. Circuits Syst. Video Technol., 23 (7), pp. 1119-1130; Pinggera, P., Breckon, T., Bischof, H., On cross-spectral stereo matching using dense gradient features (2012) Proc. of British Machine Vision Conference, pp. 5261-526:12; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Fei-Fei, L., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vis., 115 (3), pp. 211-252; Scharstein, D., Hirschmüller, H., Kitajima, Y., Krathwohl, G., Nesic, N., Wang, X., Westling, P., High-resolution stereo datasets with subpixel-accurate ground truth (2014) Proc. of German Conference on Pattern Recognition, pp. 31-42; Scharstein, D., Pal, C., Learning conditional random fields for stereo (2007) Proc. of IEEE Conference on Computer Vision and Pattern Recognition; Scharstein, D., Szeliski, R., A taxonomy and evaluation of dense two-frame stereo correspondence algorithms (2002) Int. J. Comput. Vis., 47 (1), pp. 7-42; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015) Proc. of International Conference on Learning Representations; Tombari, F., Mattocccia, S., Stefano, L.D., Addimanda, E., Classification and evaluation of cost aggregation methods for stereo correspondence (2008) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-8; Vedaldi, A., Lenc, K., MatConvnet – convolutional neural networks for MATLAB (2015) Proc. of the ACM International Conference on Multimedia, pp. 689-692; Williem, Raskar, R., Park, I.K., Depth map estimation and colorization of anaglyph images using local color prior and reverse intensity distribution (2015) Proc. of IEEE International Conference on Computer Vision, pp. 3460-3468; Xu, L., Jia, J., Depth-aware motion deblurring (2012) Proc. of IEEE International Conference on Computational Photography; Yang, Q., A non-local cost aggregation method for stereo matching (2012) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1402-1409; Yang, Q., Wang, L., Yang, R., Wang, S., Liao, M., Nistér, D., Real-time global stereo matching using hierarchical belief propagation (2006) Proc. of British Machine Vision Conference, pp. 989-998; Yoon, K.-J., Kweon, I.S., Adaptive support-weight approach for correspondence search (2006) IEEE Trans. Pattern Anal. Mach. Intell., 28 (4), pp. 650-656; Zabih, R., Woodfill, J., Non-parametric local transforms for computing visual correspondence (1994) Proc. of European Conference on Computer Vision, pp. 151-158; Zhang, K., Fang, Y., Min, D., Sun, L., Yang, S., Yan, S., Tian, Q., Cross-scale cost aggregation for stereo matching (2014) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1590-1597; Zhang, K., Lu, J., Lafruit, G., Cross-based local stereo matching using orthogonal integral images (2009) IEEE Trans. Circ. Syst. Video Technol., 19 (7), pp. 1073-1079","Park, I.K.; Department of Information and Communication Engineering, South Korea; email: pik@inha.ac.kr",,,"Elsevier B.V.",,,,,01678655,,,,"English","Pattern Recogn. Lett.",Article,"Final","",Scopus,2-s2.0-85049779130
"Wongso R., Luwinda F.A., Williem","56157356300;55990652100;51261491100;","Evaluation of Deep Super Resolution Methods for Textual Images",2018,"Procedia Computer Science","135",,,"331","337",,1,"10.1016/j.procs.2018.08.181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053150536&doi=10.1016%2fj.procs.2018.08.181&partnerID=40&md5=7c867f0fd98c7693d9c443ab16730958","Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia","Wongso, R., Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Luwinda, F.A., Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Williem, Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia","Super-resolution (SR) is one of the important pre-processing methods to refine the text images quality. Though there are numerous introduced algorithms to increase the spatial resolution for textual images, analysis on SR methods using deep learning is still insufficient. In this paper, we focus on evaluating the performance of various deep SR methods which have already confirmed to perform well in natural images super-resolution. Three evaluation metrics are used to analyze the performance of each method, such as peak signal-to-noise ratio (PSNR), structure similarity index (SSIM), and optical character recognition accuracy (OCRAcc). Experimental results show that deeper networks perform better than shallow networks for super-resolution problem. In overall, deep recursive convolutional network (DRCN) and deep laplacian pyramid network (LapSRN) alternately achieve the best performance. Then, very deep super-resolution network (VDSR) obtains the 3rd rank following both methods. © 2018 The Authors. Published by Elsevier Ltd.","benchmark; deep learning; Super-resolution","Artificial intelligence; Benchmarking; Optical character recognition; Optical resolving power; Signal to noise ratio; Convolutional networks; Evaluation metrics; Peak Signal to Noise Ratio (PSNR); Pre-processing method; Spatial resolution; Structure similarity; Super resolution; Superresolution methods; Deep learning",,,,,,,"Walha, R., Drira, F., Lebourgeois, F., Alimi, A.M., Garcia, C., Resolution enhancement of textual images: A survey of single image-based methods (2016) IET Image Processing, 10 (4), pp. 325-337; Peyrard, C., Baccouche, M., Mamalet, F., Garcia, C., ICDAR2015 competition on text image super-resolution (2015) Proc. of International Conference on Document Analysis and Recognition, pp. 1201-1205; Dong, C., Loy, C.C., He, K., Tang, X., Learning a deep convolutional network for image super-resolution (2014) Proc. of European Conference on Computer Vision, pp. 184-199; Dong, C., Loy, C.C., He, K., Tang, X., Image super-resolution using deep convolutional networks (2016) IEEE Transactions on Pattern Analysis and Machine Intelligence, 38 (2), pp. 295-307; Wang, Z., Liu, D., Yang, J., Han, W., Huang, T., Deep networks for image super-resolution with sparse prior (2015) Proc. of IEEE International Conference on Computer Vision, pp. 370-378; Kim, J., Lee, J.K., Lee, K.M., Accurate image super-resolution using very deep convolutional networks (2016) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1646-1654; Kim, J., Lee, J.K., Lee, K.M., Deeply-recursive convolutional network for image super-resolution (2016) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1637-1645; Dong, C., Loy, C.C., Tang, X., Accelerating the super-resolution convolutional neural network (2016) Proc. of European Conference on Computer Vision, pp. 391-407; Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H., Deep laplacian pyramid networks for fast and accurate super-resolution (2017) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 624-632; Walha, R., Drira, F., Lebourgeois, F., Alimi, A.M., Super-resolution of single text image by sparse representation (2012) Proc. of the Workshop on Document Analysis and Recognition, pp. 22-29; Walha, R., Drira, F., Lebourgeois, F., Garcia, C., Alimi, A.M., Multiple learned dictionaries based clustered sparse coding for the super-resolution of single text image (2013) Proc. of International Conference on Document Analysis and Recognition, pp. 484-488; Walha, R., Drira, F., Alimi, A.M., Lebourgeois, F., Garcia, C., A sparse coding based approach for the resolution enhancement and restoration of printed and handwritten textual images (2014) Proc. of International Conference on Frontiers in Handwriting Recognition, pp. 696-701; Walha, R., Drira, F., Lebourgeois, F., Garcia, C., Alimi, A.M., Resolution enhancement of textual images via multiple coupled dictionaries and adaptive sparse representation selection (2015) International Journal on Document Analysis and Recognition, 18 (1), pp. 87-101; Walha, R., Drira, F., Lebourgeois, F., Garcia, C., Alimi, A.M., Joint denoising and magnification of noisy low-resolution textual images (2015) Proc. of International Conference on Document Analysis and Recognition, pp. 871-875; Gregor, K., Lecun, Y., Learning fast approximations of sparse coding (2010) Proc. of International Conference on Machine Learning, pp. 399-406","Wongso, R.; Computer Science Department, Indonesia; email: rwongso@binus.edu","MeilianaArifin Y.Budiharto W.Wulandhari L.A.Sutoyo R.FaisalGunawan A.A.S.WilliemSuryani D.",,"Elsevier B.V.","3rd International Conference on Computer Science and Computational Intelligence, ICCSCI 2018","7 September 2018 through 8 September 2018",,138963,18770509,,,,"English","Procedia Comput. Sci.",Conference Paper,"Final","All Open Access, Gold",Scopus,2-s2.0-85053150536
"Ranteallo Sampetoding J.L., Satriyawibowo B., Williem, Wongso R., Luwinda F.A.","57203846352;57203839992;51261491100;56157356300;55990652100;","Automatic Field-of-View Expansion using Deep Features and Image Stitching",2018,"Procedia Computer Science","135",,,"657","662",,,"10.1016/j.procs.2018.08.230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053161627&doi=10.1016%2fj.procs.2018.08.230&partnerID=40&md5=e398086145b6dbec26ddedce857897ac","Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia","Ranteallo Sampetoding, J.L., Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Satriyawibowo, B., Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Williem, Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Wongso, R., Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Luwinda, F.A., Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia","Automatic photo enhancement, such field-of-view expansion, has become a challenging problem in computer graphics community. Due to the hardware limitation, image acquisition might get distracted by small field-of-view. Photo enhancement using internet photo collections has gained good performance in the past few years. However, it depends on the quality of 3D reconstruction. In this paper, we perform an automatic personal photo enhancement using the photo collection without any 3D reconstruction step. 2D global descriptor is used using NetVLAD deep architecture. Then, image stitching is applied for each similar candidate image. Experiment results show that the propose framework has promising results which could lead to further research. © 2018 The Authors. Published by Elsevier Ltd.","deep learning; field-of-view expansion; image enhancement; image stitching; netVLAD","Artificial intelligence; Computer graphics; Deep learning; Expansion; Image reconstruction; 3D reconstruction; Deep architectures; Descriptors; Field of views; Image stitching; Internet photo collections; netVLAD; Photo collections; Image enhancement",,,,,,,"Zhang, C., Gao, J., Wang, O., Georgel, P., Yang, R., Davis, J., Personal photograph enhancement using internet photo collections (2014) IEEE Trans on Visualization and Computer Graphics, 20 (2), pp. 262-275; Li, K., Wang, J., Liu, Y., Xu, L., Dai, Q., Re-compositable panoramic selfie with robust multi-frame segmentation and stitching (2016) Computer Graphics Forum, 35 (7), pp. 227-236; Arandjelović, R., Netvlad: Cnn architecture for weakly supervised place recognition (2016) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 5297-5307; Zaragoza, J., Chin, T.J., Brown, M.S., Suter, D., As-projective-as-possible image stitching with moving dlt (2013) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 2339-2346; Yan, Z., Zhang, H., Wang, B., Paris, S., Yu, Y., Automatic photo adjustment using deep neural networks (2016) ACM Trans on Graphics, 35 (2), pp. 111-1115; Zhu, F., Yan, Z., Bu, J., Yu, Y., Exemplar-based image and video stylization using fully convolutional semantic features (2017) IEEE Trans on Image Processing, 26 (7), pp. 3542-3555; Joshi, N., Matusik, W., Adelson, E.H., Kriegman, D.J., Personal photo enhancement using example images (2010) ACM Trans on Graphics, 29 (2), pp. 121-1215; Shan, Q., Curless, B., Furukawa, Y., Hernandez, C., Seitz, S.M., Photo uncrop (2014) Proc. of European Conference on Computer Vision, pp. 16-31; Philbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A., Object retrieval with large vocabularies and fast spatial matching (2007) Proc. of IEEE Conference on Computer Vision and Pattern Recognition; Jégou, H., Aggregating local descriptors into a compact image representation (2010) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 3304-3311; Perronnin, F., Large-scale image retrieval with compressed fisher vectors (2010) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 3384-3391; Gao, J., Kim, S.J., Brown, M.S., Constructing image panoramas using dual-homography warping (2011) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 49-56; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014) CoRR, , abs/1409.1556; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) International Journal of Computer Vision, 60 (2), pp. 91-110; Levin, A., Zomet, A., Peleg, S., Weiss, Y., Seamless image stitching in the gradient domain (2004) Proc. of European Conference on Computer Vision, pp. 377-389","Williem; Computer Science Department, Indonesia; email: williem003@binus.ac.id","MeilianaArifin Y.Budiharto W.Wulandhari L.A.Sutoyo R.FaisalGunawan A.A.S.WilliemSuryani D.",,"Elsevier B.V.","3rd International Conference on Computer Science and Computational Intelligence, ICCSCI 2018","7 September 2018 through 8 September 2018",,138963,18770509,,,,"English","Procedia Comput. Sci.",Conference Paper,"Final","All Open Access, Gold",Scopus,2-s2.0-85053161627
"Williem, Ivan A., Seok H., Lim J., Yoon K.-J., Cho I., Park I.K.","51261491100;57202220044;57202213150;7403453968;55932200600;57202219357;8612277600;","Visual-inertial RGB-D SLAM for mobile augmented reality",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10736 LNCS",,,"928","938",,2,"10.1007/978-3-319-77383-4_91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047444903&doi=10.1007%2f978-3-319-77383-4_91&partnerID=40&md5=a4923540430f1f6893d71ac67e379c6a","Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Division of Computer Science, Hanyang University, Seoul, South Korea; School of Electrical Engineering and Computer Science, GIST, Gwangju, South Korea; Media Experience Lab, Corporate R&D Center, SK Telecom, Seoul, South Korea","Williem, Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Ivan, A., Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Seok, H., Division of Computer Science, Hanyang University, Seoul, South Korea; Lim, J., Division of Computer Science, Hanyang University, Seoul, South Korea; Yoon, K.-J., School of Electrical Engineering and Computer Science, GIST, Gwangju, South Korea; Cho, I., Media Experience Lab, Corporate R&D Center, SK Telecom, Seoul, South Korea; Park, I.K., Department of Information and Communication Engineering, Inha University, Incheon, South Korea","This paper presents a practical framework for occlusion-aware augmented reality application using visual-inertial RGB-D SLAM. First, an efficient visual SLAM framework with map merging based relocalization is introduced. When the pose estimation fails, a new environment map is generated. Then, a map merging is performed to merge the current and previous environment maps if a loop closure is detected. The framework is then integrated with the inertial information to solve the missing environment map problem. Camera pose is approximated using the angular velocity and translational acceleration value when the pose estimation fails. Experimental results show that the proposed method can perform well in the presence of missing pose. Finally, an occlusion-aware augmented reality application is built over the SLAM framework. © Springer International Publishing AG, part of Springer Nature 2018.",,"Merging; Augmented reality applications; Environment maps; Loop closure; Map merging; Mobile augmented reality; Pose estimation; Re-localization; Visual SLAM; Augmented reality",,,,"Ministry of Science and ICT, South Korea, MSIT: 2017-0-00142

Institute for Information and Communications Technology Promotion, IITP

Institute for Information and Communications Technology Promotion, IITP","Acknowledgement. This work was supported by SK Telecom. This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (2017-0-00142).",,"Brunetto, N., Salti, S., Fioraio, N., Cavallari, T., Stefano, L.D., Fusion of inertial and visual measurements for RGB-D SLAM on mobile devices (2015) Proceedings of IEEE International Conference on Computer Vision Workshops, pp. 148-156; Davison, A.J., Reid, I.D., Molton, N.D., Stasse, O., MonoSLAM: Real-time single camera SLAM (2007) IEEE Trans. Pattern Anal. Mach. Intell., 29 (6), pp. 1052-1067; Fioraio, N., Di Stefano, L., SlamDunk: Affordable real-time RGB-D SLAM (2015) ECCV 2014. LNCS, 8925, pp. 401-414. , https://doi.org/10.1007/978-3-319-16178-5_28, Agapito, L., Bronstein, M.M., Rother, C. (eds.), Springer, Cham; Galvez-Lopez, D., Tardos, J.D., Real-time loop detection with bags of binary words (2011) Proceedings of IEEE International Conference on Intelligent Robots and Systems, pp. 51-58; Henry, P., Krainin, M., Herbst, E., Ren, X., Fox, D., RGB-D mapping: Using kinect-style depth cameras for dense 3D modeling of indoor environments (2012) Int. J. Robot. Res., 31 (5), pp. 647-663; Kerl, C., Sturm, J., Cremers, D., Dense visual SLAM for RGB-D cameras (2013) Proceedings of IEEE International Conference on Intelligent Robotics and Systems, pp. 2100-2106; Klein, G., Murray, D., Improving the agility of keyframe-based SLAM (2008) ECCV 2008. LNCS, 5303, pp. 802-815. , https://doi.org/10.1007/978-3-540-88688-4_59, Forsyth, D., Torr, P., Zisserman, A. (eds.), Springer, Heidelberg; Kneip, L., Scaramuzza, D., Siegwart, R., A novel parametrization of the perspective-three-point problem for a direct computation of absolute camera position and orientation (2011) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 2969-2976; Leutenegger, S., Lynen, S., Bosse, M., Siegwart, R., Furgale, P., Keyframe-based visualinertial odometry using nonlinear optimization (2015) Int. J. Robot. Res., 34 (3), pp. 314-334; Mahony, R., Hamel, T., Pflimlin, J.M., Nonlinear complementary filters on the special orthogonal group (2008) IEEE Trans. Autom. Control, 53 (5), pp. 1203-1217; Marroquim, R., Kraus, M., Rcavalcanti, P., Efficient point-based rendering using image reconstruction (2007) Proceedings of Eurographgics Symposium on Point-Based Graphics, pp. 101-108; Mur-Artal, R., Montiel, J.M.M., Tardós, J.D., ORB-SLAM: A versatile and accurate monocular SLAM system (2015) IEEE Trans. Robot., 31 (5), pp. 1147-1163; Newcombe, R.A., Lovegrove, S.J., Davison, A.J., DTAM: Dense tracking and mapping in real-time (2011) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 2320-2327; Nister, D., Stewenius, H., Scalable recognition with a vocabulary tree (2006) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 2161-2168; Servant, F., Houlier, P., Marchand, E., Improving monocular plane-based SLAM with inertial measures (2010) Proceedings of International Conference on Intelligent Robots and Systems, pp. 3810-3815; Shi, J., Tomasi, C., Good features to track (1994) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 593-600; Sünderhauf, N., Protzel, P., Towards a robust back-end for pose graph SLAM (2012) Proceedings of IEEE International Conference on Robotics and Automation, pp. 1254-1261; Tedaldi, D., Pretto, A., Menegatti, E., A robust and easy to implement method for IMU calibration without external equipments (2014) Proceedings of IEEE International Conference on Robotics and Automation, pp. 3042-3049; Tiefenbacher, P., Schulze, T., Rigoll, G., Off-the-shelf sensor integration for mono- SLAM on smart devices (2015) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 15-20","Park, I.K.; Department of Information and Communication Engineering, South Korea; email: pik@inha.ac.kr","Zeng B.Li H.Huang Q.El Saddik A.Jiang S.Fan X.","","Springer Verlag","18th Pacific-Rim Conference on Multimedia, PCM 2017","28 September 2017 through 29 September 2017",,213449,03029743,9783319773827,,,"English","Lect. Notes Comput. Sci.",Conference Paper,"Final","",Scopus,2-s2.0-85047444903
"Ong V., Rahmanto A.D.S., Williem W., Suhartono D., Nugroho A.E., Andangsari E.W., Suprayogi M.N.","57193890656;57193890326;51261491100;55834784000;54080199100;57191035003;57195954440;","Personality prediction based on Twitter information in Bahasa Indonesia",2017,"Proceedings of the 2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017",,, 8104567,"367","372",,14,"10.15439/2017F359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039911733&doi=10.15439%2f2017F359&partnerID=40&md5=882a5ef32c34ccb2edc2d165788882c5","School of Computer Science, Computer Science Department, Bina Nusantara University, Jakarta, Indonesia; Faculty of Humanities, Psychology Department, Bina Nusantara University, Jakarta, Indonesia","Ong, V., School of Computer Science, Computer Science Department, Bina Nusantara University, Jakarta, Indonesia; Rahmanto, A.D.S., School of Computer Science, Computer Science Department, Bina Nusantara University, Jakarta, Indonesia; Williem, W., School of Computer Science, Computer Science Department, Bina Nusantara University, Jakarta, Indonesia; Suhartono, D., School of Computer Science, Computer Science Department, Bina Nusantara University, Jakarta, Indonesia; Nugroho, A.E., Faculty of Humanities, Psychology Department, Bina Nusantara University, Jakarta, Indonesia; Andangsari, E.W., Faculty of Humanities, Psychology Department, Bina Nusantara University, Jakarta, Indonesia; Suprayogi, M.N., Faculty of Humanities, Psychology Department, Bina Nusantara University, Jakarta, Indonesia","The sheer usage of social media presents an opportunity for an automated analysis of a social media user based on his/her information, activities, or status updates. This opportunity is due to the abundant amount of information shared by the user. This fact is especially true for countries with high number of active social media users such as Indonesia. Extraction of information from social media can yield insightful results if done correctly. Recent studies have managed to leverage associations between language and personality and build a personality prediction system based on those associations. The current study attempts to build a personality prediction system based on a Twitter user's information for Bahasa Indonesia, the native language of Indonesia. The personality prediction system is built on Support Vector Machine and XGBoost trained with 329 instances (users). Evaluation results using 10-fold cross validation shows that the system managed to reach highest average accuracy of 76.2310% with Support Vector Machine and 97.9962% with XGBoost. © 2017 PTI.",,"Forecasting; Information systems; Support vector machines; 10-fold cross-validation; Amount of information; Automated analysis; Evaluation results; Extraction of information; Native language; Personality predictions; Status updates; Social networking (online)",,,,"Kementerian Riset, Teknologi dan Pendidikan Tinggi

Kementerian Riset, Teknologi dan Pendidikan Tinggi: 039A/VR","This work was supported by grant from Ministry of Research, Technology and Higher Education of the Republic of Indonesia.","This research and publication is fully supported by grant named “Penelitian Produk Terapan” from Ministry of Research, Technology and Higher Education of the Republic of Indonesia with contract number 039A/VR.RTT/VI/2017","(2016) Global Web Index Social Report Q4/2016, , GlobalWebIndex; (2014) Q414 Selected Company Metrics and Financials, , Twitter Investor Relations; (2016) Q216 Selected Company Metrics and Financials, , Twitter Investor Relations; Carley, K.M., Malik, M.M., Kowalchuck, M., Pfeffer, J., Landwehr, P., (2015) Twitter Usage in Indonesia; (2016) Twitter Rahasiakan Jumlah Pengguna di Indonesia, , CNN Indonesia; (2015) Southeast Asia Has among the Highest Social Network Usage in the World, , eMarketer; Golbeck, J., Robles, C., Edmondson, M., Turner, K., Predicting personality from twitter (2011) Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference On Social Computing (SocialCom), 2011 IEEE Third International Conference On, pp. 149-156; Wijaya, A., Prasetia, I., Febrianto, N., Suhartono, D., (2016) Sistem Prediksi Kepribadian 'The Big Five Traits' Dari Data Twitter, , Bina Nusantara University; Sumner, C., Byers, A., Boochever, R., Park, G.J., Predicting dark triad personality traits from twitter usage and a linguistic analysis of tweets (2012) Machine Learning and Applications (ICMLA), 2012 11th International Conference On, 2, pp. 386-393; Farnadi, G., Zoghbi, S., Moens, M., De Cock, M., Recognising personality traits using facebook status updates (2013) Work. Comput. Personal. Recognit. Int. AAAI Conf. Weblogs Soc. Media, pp. 14-18; Arroju, M., Hassan, A., Farnadi, G., Age, gender and personality recognition using tweets in a multilingual setting (2015) 6th Conference and Labs of the Evaluation Forum (CLEF 2015): Experimental IR Meets Multilinguality, Multimodality, Interaction; Wan, D., Zhang, C., Wu, M., An, Z., (2014) Personality Prediction Based On All Characters of User Social Media Information, pp. 220-230; Ong, V., Rahmanto, A.D.S., Suhartono, W.D., Exploring personality prediction from text on social media: A literature review (2017) Internetworking Indones. J., 9 (1), pp. 65-70; Iacobelli, F., Gill, A.J., Nowson, S., Oberlander, J., Large scale personality classification of bloggers (2011) Affective Computing and Intelligent Interaction: Fourth International Conference, ACII 2011, pp. 568-577. , Memphis, TN, USA, October 9-12, 2011, Proceedings, Part II, S. D'Mello, A. Graesser, B. Schuller, J.-C. Martin, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg; Yarkoni, T., Personality in 100, 000 words: A large-scale analysis of personality and word use among bloggers (2010) J. Res. Pers., 44 (3), pp. 363-373; Schwartz, H.A., Eichstaedt, J.C., Kern, M.L., Dziurzynski, L., Ramones, S.M., Agrawal, M., Shah, A., Ungar, L.H., (2013) Personality, Gender, Age in the Language of Social Media : The Open-Vocabulary Approach, 8 (9); Liu, Y., Wang, J., Jiang, Y., PT-LDA: A latent variable model to predict personality traits of social network users (2015) Neurocomputing; Peng, K.-H., Liou, L.-H., Chang, C.-S., Lee, D.-S., Predicting personality traits of Chinese users based on Facebook wall posts (2015) Wireless and Optical Communication Conference (WOCC), 2015 24th, pp. 9-14; Pratama, B.Y., Sarno, R., Personality classification based on Twitter text using Naive Bayes, KNN and SVM (2015) 2015 International Conference On Data and Software Engineering (ICoDSE), pp. 170-174; Amichai-Hamburger, Y., Vinitzky, G., Social network use and personality (2010) Comput. Human Behav., 26 (6), pp. 1289-1295; Skues, J.L., Williams, B., Wise, L., The effects of personality traits, self-esteem, loneliness, narcissism on Facebook use among university students (2012) Comput. Human Behav., 28 (6), pp. 2414-2419; Ryan, T., Xenos, S., Who uses Facebook An investigation into the relationship between the Big Five, shyness, narcissism, loneliness, Facebook usage (2011) Comput. Human Behav., 27 (5), pp. 1658-1664; Correa, T., Hinsley, A.W., De Zuniga, H.G., Who interacts on the Web: The intersection of users' personality and social media use (2010) Comput. Human Behav., 26 (2), pp. 247-253; Ross, C., Orr, E.S., Sisic, M., Arseneault, J.M., Simmering, M.G., Orr, R.R., Personality and motivations associated with Facebook use (2009) Comput. Human Behav., 25 (2), pp. 578-586; McCrae, R.R., John, O.P., An introduction to the fivefactor model and its applications (1992) J. Pers., 60 (2), pp. 175-215; Weiner, I.B., Greene, R.L., Revised NEO personality inventory (2008) Handb. Personal. Assess., pp. 315-342; Naradhipa, A.R., Purwarianti, A., Sentiment classification for Indonesian message in social media (2012) Cloud Computing and Social Networking (ICCCSN), 2012 International Conference On, pp. 1-5; Buntoro, G.A., Adji, T.B., Purnamasari, A.E., (2014) Sentiment Analysis Twitter Dengan Kombinasi Lexicon Based Dan Double Propagation, pp. 7-8; Tala, F.Z., (2003) A Study of Stemming Effects On Information Retrieval in Bahasa Indonesia, , Inst. Logic, Lang. Comput. Univ. van Amsterdam, Netherlands; Rehurek, R., Sojka, P., Software framework for topic modelling with large corpora (2010) Proc. Lr. 2010 Work. New Challenges NLP Fram., pp. 45-50; Ayumi, V., Pose-based human action recognition with extreme gradient boosting (2016) Research and Development (SCOReD), 2016 IEEE Student Conference On, pp. 1-5; Babajide Mustapha, I., Saeed, F., Bioactive molecule prediction using extreme gradient boosting (2016) Molecules, 21 (8), p. 983; Dey, S., Kumar, Y., Saha, S., Basak, S., Forecasting to Classification: Predicting the direction of stock market price using Xtreme Gradient Boosting; Chen, T., Guestrin, C., Xgboost: A scalable tree boosting system (2016) Proceedings of the 22nd Acm Sigkdd International Conference On Knowledge Discovery and Data Mining, pp. 785-794",,"Ganzha M.Maciaszek L.Paprzycki M.","","Institute of Electrical and Electronics Engineers Inc.","2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017","3 September 2017 through 6 September 2017",,131962,,9788394625375,,,"English","Proc. Fed. Conf. Comput. Sci. Inf. Syst., FedCSIS",Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85039911733
"Williem W., Park I.K.","51261491100;8612277600;","Robust Light Field Depth Estimation for Noisy Scene with Occlusion",2016,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2016-December",, 7780845,"4396","4404",,47,"10.1109/CVPR.2016.476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986302057&doi=10.1109%2fCVPR.2016.476&partnerID=40&md5=14171b423c85ef7f795cf37e56d61955","Dept. of Information and Communication Engineering, Inha University, South Korea","Williem, W., Dept. of Information and Communication Engineering, Inha University, South Korea; Park, I.K., Dept. of Information and Communication Engineering, Inha University, South Korea","Light field depth estimation is an essential part of many light field applications. Numerous algorithms have been developed using various light field characteristics. However, conventional methods fail when handling noisy scene with occlusion. To remedy this problem, we present a light field depth estimation method which is more robust to occlusion and less sensitive to noise. Novel data costs using angular entropy metric and adaptive defocus response are introduced. Integration of both data costs improves the occlusion and noise invariant capability significantly. Cost volume filtering and graph cut optimization are utilized to improve the accuracy of the depth map. Experimental results confirm that the proposed method is robust and achieves high quality depth maps in various scenes. The proposed method outperforms the state-of-the-art light field depth estimation methods in qualitative and quantitative evaluation. © 2016 IEEE.",,"Computer vision; Costs; Graphic methods; Conventional methods; Depth Estimation; Entropy metric; High quality; Light fields; Noisy scenes; Quantitative evaluation; State of the art; Pattern recognition",,,,,,,"Bleyer, M., Rother, C., Kohli, P., Surface stereo with soft segmentation (2010) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 1570-1577; Bok, Y., Jeon, H.-G., Kweon, I.S., Geometric calibration of micro-lens-based light-field cameras using line features (2014) Proc. of European Conference on Computer Vision, pp. 47-61; Boykov, Y., Veksler, O., Zabih, R., Fast approximate energy minimization via graph cuts (2001) IEEE Trans. on Pattern Analysis and Machine Intelligence, 23 (11), pp. 1222-1239. , Nov; Chen, C., Lin, H., Yu, Z., Kang, S.B., Yu, J., Light field stereo matching using bilateral statistics of surface cameras (2014) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 1518-1525; Cho, D., Kim, S., Tai, Y.-W., Consistent matting for light field images (2014) Proc. of European Conference on Computer Vision, pp. 90-104; Cho, D., Lee, M., Kim, S., Tai, Y.-W., Modeling the calibration pipeline of the Lytro camera for high quality lightfield image reconstruction (2013) Proc. of IEEE International Conference on Computer Vision, pp. 3280-3287; Dansereau, D.G., Pizarro, O., Williams, S.B., Decoding, calibration and rectification for lenselet-based plenoptic cameras (2013) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 1027-1034; He, K., Sun, J., Tang, X., Guided image filtering (2013) IEEE Trans. on Pattern Analysis and Machine Intelligence, 35 (6), pp. 1397-1409. , June; Hosni, A., Rhemann, C., Bleyer, M., Rother, C., Gelautz, M., Fast cost-volume filtering for visual correspondence and beyond (2013) IEEE Trans. on Pattern Analysis and Machine Intelligence, 35 (2), pp. 504-511. , Feb; Jarabo, A., Masia, B., Bousseau, A., Pellacini, F., Gutierrez, D., How do people edit light fields (2014) ACM Trans. on Graphics, 33 (4). , July; Jeon, H.G., Park, J., Choe, G., Park, J., Bok, Y., Tai, Y.W., Kweon, I.S., Accurate depth map estimation from a lenslet light field camera (2015) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 1547-1555; Kang, S.B., Szeliski, R., Chai, J., Handling occlusions in dense multi-view stereo (2001) Proc. of IEEE Computer Vision and Pattern Recognition; Kolmogorov, V., Zabih, R., Multi-camera scene reconstruction via graph cuts (2002) Proc. of European Conference on Computer Vision, pp. 82-96; Li, N., Ye, J., Ji, Y., Ling, H., Yu, J., Saliency detection on light field (2014) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 2806-2813; Lin, H., Chen, C., Kang, S.B., Yu, J., Depth recovery from light field using focal stack symmetry (2015) Proc. of IEEE International Conference on Computer Vision; Lytro, (2014) The Lytro Camera; Ng, R., Fourier slice photography (2005) ACM Trans. on Graphics, 24 (3), pp. 735-744. , July; (2013) 3D Light Field Camera Technology, , Raytrix; Tao, M., Hadap, S., Malik, J., Ramamoorthi, R., Depth from combining defocus and correspondence using lightfield cameras (2013) Proc. of IEEE International Conference on Computer Vision, pp. 673-680; Tao, M., Srinivasan, P.P., Malik, J., Rusinkiewicz, S., Ramamoorthi, R., Depth from shading, defocus, and correspondence using light-field angular coherence (2015) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 1940-1948; Vaish, V., Levoy, M., Szeliski, R., Zitnick, C.L., Kang, S.B., Reconstructing occluded surfaces using synthetic apertures: Stereo, focus and robust measures (2006) Proc. of IEEE Computer Vision and Pattern Recognition; Wang, T.C., Efros, A.A., Ramamoorthi, R., Occlusionaware depth estimation using light-field cameras (2015) Proc. of IEEE International Conference on Computer Vision; Wanner, S., Goldluecke, B., Globally consistent depth labelling of 4D lightfields (2012) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 41-48; Wanner, S., Meister, S., Goldluecke, B., Datasets and benchmarks for densely sampled 4D light fields (2013) Proc. of Vision, Modeling & Visualization, pp. 225-226; Wei, Y., Quan, L., Asymmetrical occlusion handling using graph cut for multi-view stereo (2005) Proc. of IEEE Computer Vision and Pattern Recognition, pp. 902-909; Wilburn, B., Joshi, N., Vaish, V., Talvala, E., Artunez, E., Barth, A., Adams, A., Levoy, M., High perofrmance imaging using large camera arrays (2005) ACM Trans. on Graphics, 24 (3), pp. 765-779. , July",,,"","IEEE Computer Society","29th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016","26 June 2016 through 1 July 2016",,125363,10636919,9781467388504,PIVRE,,"English","Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit",Conference Paper,"Final","",Scopus,2-s2.0-84986302057
"Williem, Shon K.W., Park I.K.","51261491100;57190273458;8612277600;","Spatio-angular consistent editing framework for 4D light field images",2016,"Multimedia Tools and Applications","75","23",,"16615","16631",,11,"10.1007/s11042-016-3754-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978891122&doi=10.1007%2fs11042-016-3754-y&partnerID=40&md5=c1d411e707688ad056ec77056eb113cd","Department of Information and Communication Engineering, Inha University, Incheon, South Korea","Williem, Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Shon, K.W., Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Park, I.K., Department of Information and Communication Engineering, Inha University, Incheon, South Korea","This paper presents a practical framework for various light field editing algorithms, such as segmentation, sparse edit propagation, and inpainting. A novel framework is introduced with its user interface to perform the light field editing algorithms. Unlike single-image algorithms, the coherence between light field subaperture images is fully considered. Instead of processing all light field subaperture images independently, the proposed framework performs editing in the cluster image domain. The edit result in the cluster image is propagated back to each light field subaperture image using 2D-to-4D light field edit propagation. Experimental results on test images captured by a Lytro off-the-shelf light field camera confirm that the proposed method provides robust and consistent results of edited light field subaperture images. © 2016, Springer Science+Business Media New York.",,"Algorithms; Image segmentation; Edit propagation; Image domain; Inpainting; Light fields; Single images; Spatio-angular consistent; Subaperture; Test images; User interfaces",,,,,,,"An, X., Pellacini, F., Appprop: all-pairs appearance-space edit propagation (2008) ACM Trans Graph, 27 (3), pp. 40:1-40:9; Ao, H., Zhang, Y., Jarabo, A., Masia, B., Liu, Y., Gutierrez, D., Dai, Q., Light field editing based on reparameterization. In: Proceedings of the 16th pacific-rim conference on multimedia (2015) pp 601–610; Bie, X., Huang, H., Wang, W., Real time edit propagation by efficient sampling (2011) Comput Graphics Forum, 30 (7), pp. 2041-2048; Bok, Y., Jeon, H.G., Kweon, I.S., Geometric calibration of micro-lens-based light-field cameras using line features. In: Proceedings of the european conference on computer vision (ECCV) (2014) pp 47–61; Chen, C., Lin, H., Yu, Z., Kang, S.B., Yu, J., Light field stereo matching using bilateral statistics of surface cameras. In: Proceedings of IEEE computer vision and pattern recognition (2014) pp 1518–1525; Chen, X., Zou, D., Zhao, Q., Tan, P., Manifold preserving edit propagation (2012) ACM Trans Graph, 31 (6), pp. 132:1-132:7; Cho, D., Kim, S., Tai, Y.W., Consistent matting for light field images. In: Proceedings of the european conference on computer vision (ECCV) (2014) pp 90–104; Cho, D., Lee, M., Kim, S., Tai, Y.W., Modeling the calibration pipeline of the lytro camera for high quality light-field image reconstruction. In: Proceedings of IEEE international conference on computer vision (2013) pp 3280–3287; Criminisi, A., Perez, P., Toyama, K., Region filling and object removal by exemplar-based inpainting (2004) IEEE Trans Image Process, 13 (9), pp. 1200-1212; Criminisi, A., Sharp, T., Rother, C., P’erez, P., Geodesic image and video editing (2010) ACM Trans Graph, 29 (5), pp. 134:1-134:15; Dansereau, D.G., Pizarro, O., Williams, S.B., Decoding, calibration and rectification for lenselet-based plenoptic cameras. In: Proceedings of IEEE computer vision and pattern recognition (2013) pp 1027–1034; Gastal, E.S.L., Oliveira, M.M., Domain transform for edge-aware image and video processing (2011) ACM Trans Graph, 30 (4), pp. 69:1-69:11; Gortler, S.J., Grzeszczuk, R., Szeliski, R., Cohen, M.F., The lumigraph. In: Proceedings of SIGGRAPH (1996) pp 43–54; Jarabo, A., Masia, B., Bousseau, A., Pellacini, F., Gutierrez, D., How do people edit light fields? (2014) ACM Trans Graph, 33 (4); Jarabo, A., Masia, B., Gutierrez, D., Efficient propagation of light field edits. In: Proceedings of the fifth ibero-american symposium in computer graphics (2011) pp 75–80; Lang, M., Wang, O., Aydin, T., Smolic, A., Gross, M., Practical temporal consistency for image-based graphics applications (2012) ACM Trans Graph, 31 (4), pp. 34:1-34:8; Levin, A., Lischinski, D., Weiss, Y., Colorization using optimization (2004) ACM Trans Graph, 23 (3), pp. 689-694; Levin, A., Lischinski, D., Weiss, Y., A closed-form solution to natural image matting (2008) IEEE Trans Pattern Anal Mach Intell, 30 (2), pp. 228-242; Levoy, M., Hanrahan, P., Light field rendering. In: Proceedings of SIGGRAPH (1996) pp 31–42; Li, N., Ye, J., Ji, Y., Ling, H., Yu, J., Saliency detection on light field. In: Proceedings of IEEE computer vision and pattern recognition (2014) pp 2806–2813; Li, Y., Ju, T., Hu, S.M., Instant propagation of sparse edits on images and videos (2010) Comput Graphics Forum, 29 (7), pp. 2049-2054; Lischinski, D., Farbman, Z., Uyttendaele, M., Szeliski, R., Interactive local adjusment of tonal values (2006) ACM Trans Graph, 25 (3), pp. 646-653; https://www.lytro.com, Lytro (2014) The Lytro camera; Ng, R., Levoy, M., Bredif, M., Duval, G., Horowitz, M., Hanrahan, P., Light field photography with a hand-held plenoptic camera (2005) In: Stanford tech report CTSR 2005-02; www.raytrix.de, Raytrix (2013) 3D light field camera technology; Tang, M., Gorelick, L., Veksler, O., Boykov, Y., Grabcut in one cut. In: Proceedings of IEEE international conference on computer vision (2013) pp 1769–1776; Tao, M., Hadap, S., Malik, J., Ramamoorthi, R., Depth from combining defocus and correspondence using light-field cameras. In: Proceedings of IEEE international conference on computer vision (2013) pp 673–680; Wilburn, B., Joshi, N., Vaish, V., Talvala, E., Artunez, E., Barth, A., Adams, A., Levoy, M., High perofrmance imaging using large camera arrays (2005) ACM Trans Graph, 24 (3), pp. 765-779; Xu, K., Li, Y., Ju, T., Hu, S.M., Liu, T.Q., Efficient affinity-based edit propagation using K-D tree (2009) ACM Trans Graph, 28 (5), pp. 118:1-118:6","Park, I.K.; Department of Information and Communication Engineering, South Korea; email: pik@inha.ac.kr",,,"Springer New York LLC",,,,,13807501,,MTAPF,,"English","Multimedia Tools Appl",Article,"Final","",Scopus,2-s2.0-84978891122
"Williem, Raskar R., Park I.K.","51261491100;6602886524;8612277600;","Depth map estimation and colorization of anaglyph images using local color prior and reverse intensity distribution",2015,"Proceedings of the IEEE International Conference on Computer Vision","2015 International Conference on Computer Vision, ICCV 2015",, 7410752,"3460","3468",,7,"10.1109/ICCV.2015.395","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973883005&doi=10.1109%2fICCV.2015.395&partnerID=40&md5=3fe95661a7639382959e817e83c4f4e2","Inha University, Incheon, 402-751, South Korea; MIT Media Lab, Cambridge, MA  02139, United States","Williem, Inha University, Incheon, 402-751, South Korea; Raskar, R., MIT Media Lab, Cambridge, MA  02139, United States; Park, I.K., Inha University, Incheon, 402-751, South Korea","In this paper, we present a joint iterative anaglyph stereo matching and colorization framework for obtaining a set of disparity maps and colorized images. Conventional stereo matching algorithms fail when addressing anaglyph images that do not have similar intensities on their two respective view images. To resolve this problem, we propose two novel data costs using local color prior and reverse intensity distribution factor for obtaining accurate depth maps. To colorize an anaglyph image, each pixel in one view is warped to another view using the obtained disparity values of non-occluded regions. A colorization algorithm using optimization is then employed with additional constraint to colorize the remaining occluded regions. Experimental results confirm that the proposed unified framework is robust and produces accurate depth maps and colorized stereo images. © 2015 IEEE.",,"Algorithms; Computer vision; Iterative methods; Optimization; Stereo image processing; Depth map estimation; Disparity map; Intensity distribution; Local color; Stereo matching; Stereo matching algorithm; Stereo-image; Unified framework; Image matching",,,,,,,"Mobile 3DTV Content Delivery Optimization over DVB-H System, , http://sp.cs.tut.fi/mobile3dtv/stereo-video/; Bando, Y., Chen, B.-Y., Nishita, T., Extracting depth and matte using a color-filtered aperture (2008) ACM Trans. on Graphics, 27 (5), p. 134. , Dec; Boykov, Y., Veksler, O., Zabih, R., Fast approximate energy minimization via graph cuts (2001) IEEE Trans. on Pattern Analysis and Machine Intelligence, 23 (11), pp. 1222-1239. , Nov; Chen, X., Zou, D., Zhao, Q., Tan, P., Manifold preserinvg edit propagation (2012) ACM Trans. on Graphics, 31 (6), pp. 1321-1327. , Nov; Gastal, E.S.L., Oliveira, M.M., Domain transform for edge-aware image and video processing (2011) ACM Trans. on Graphics, 30 (4), pp. 691-6912. , July; Heo, Y.S., Lee, K.M., Lee, S.U., Robust stereo matching using adaptive normalized cross correlation (2011) IEEE Trans. on Pattern Analysis and Machine Intelligence, 33 (4), pp. 807-822. , Apr; Heo, Y.S., Lee, K.M., Lee, S.U., Joint depth map and color consistency estimation for stereo images with different illuminations and cameras (2013) IEEE Trans. on Pattern Analysis and Machine Intelligence, 35 (5), pp. 1094-1106. , May; Hirschmüller, H., Scharstein, D., Evaluation of stereo matching costs on images with radiometric differences (2009) IEEE Trans. on Pattern Analysis and Machine Intelligence, 31 (9), pp. 1582-1599. , Sept; Hong, L., Chen, G., Segment-based stereo matching using graph cuts (2004) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. I74-I81; Joulin, A., Kang, S.B., Recovering stereo pairs from anaglyphs (2013) Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 289-296; Kim, J., Kolmogorov, V., Zabih, R., Visual correspondence using energy minimization and mutual information (2003) Proc. of IEEE International Conference on Computer Vision, pp. 1033-1040; Levin, A., Lischinski, D., Weiss, Y., Colorization using optimization (2004) ACM Trans. on Graphics, 23 (3), pp. 689-694. , Aug; Lin, H.S., Zheng, C.L., Lin, Y.H., Ouhyoung, M., Optimized anaglyph colorization (2012) Proc. of SIGGRAPH Asia 2010 Technical Briefs; Liu, C., Yuen, J., Torralba, A., Sivic, J., Freeman, A.T., Sift flow: Dense correspondence across different scenes (2008) Proc. of the European Conference on Computer Vision (ECCV): Part III, pp. 28-42; Meltzer, T., Yanover, C., Weiss, Y., Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation (2005) Proc. of IEEE International Conference on Computer Vision, pp. 428-435; Reinhard, E., Adhikhmin, M., Gooch, B., Shirley, P., Color transfer between images (2001) IEEE Computer Graphics and Applications, 21 (5), pp. 34-41. , Sept; Scharstein, D., Szeliski, R., Middlebury Stereo Vision Page., , http://vision.middlebury.edu/stereo/; Scharstein, D., Szeliski, R., A taxonomy and evaluation of dense two-frame stereo correspondence algorithms (2002) International Journal of Computer Vision, 47 (1), pp. 7-42. , May; Yatziv, L., Sapiro, G., Fast image and video colorization using chrominance blending (2006) IEEE Trans. on Image Processing, 15 (5), pp. 1120-1129. , May; Yoon, K.-J., Kweon, I.S., Adaptive support-weight approach for correspondence search (2006) IEEE Trans. on Pattern Analysis and Machine Intelligence, 28 (4), pp. 650-656. , Apr; Zabih, R., Woodfill, J., Non-parametric local transforms for computing visual correspondence (1994) Proc. of European Conference on Computer Vision, pp. 151-158",,,"","Institute of Electrical and Electronics Engineers Inc.","15th IEEE International Conference on Computer Vision, ICCV 2015","11 December 2015 through 18 December 2015",,119541,15505499,9781467383912,PICVE,,"English","Proc IEEE Int Conf Comput Vision",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84973883005
"Simon C., Williem, Park I.K.","56179982100;51261491100;8612277600;","Correcting geometric and photometric distortion of document images on a smartphone",2015,"Journal of Electronic Imaging","24","1", 13038,"","",,9,"10.1117/1.JEI.24.1.013038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924158810&doi=10.1117%2f1.JEI.24.1.013038&partnerID=40&md5=813bd3dc2ebc2c78d2bb98cf43a565ef","Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon, 402-751, South Korea","Simon, C., Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon, 402-751, South Korea; Williem, Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon, 402-751, South Korea; Park, I.K., Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon, 402-751, South Korea","A set of document image processing algorithms for improving the optical character recognition (OCR) capability of smartphone applications is presented. The scope of the problem covers the geometric and photometric distortion correction of document images. The proposed framework was developed to satisfy industrial requirements. It is implemented on an off-the-shelf smartphone with limited resources in terms of speed and memory. Geometric distortions, i.e., skew and perspective distortion, are corrected by sending horizontal and vertical vanishing points toward infinity in a downsampled image. Photometric distortion includes image degradation from moiré pattern noise and specular highlights. Moiré pattern noise is removed using lowpass filters with different sizes independently applied to the background and text region. The contrast of the text in a specular highlighted area is enhanced by locally enlarging the intensity difference between the background and text while the noise is suppressed. Intensive experiments indicate that the proposed methods show a consistent and robust performance on a smartphone with a runtime of less than 1 s. © 2015 SPIE and IS&T.","document image; moiré pattern noise; optical character recognition; perspective distortion; photometric distortion; smartphone; specular highlight","Algorithms; Character recognition; Geometry; Image processing; Low pass filters; Optical character recognition; Optical data processing; Photometry; Signal encoding; Document images; Pattern noise; Perspective distortion; Photometric distortions; Specular highlight; Smartphones",,,,"National Research Foundation of Korea, NRF: NRF-2013R1A2A2A01069181",,,"Laine, M., Nevalainen, O.S., A standalone OCR system for mobile cameraphones (2006) Proc. of IEEE Int. Symposium on Personal, Indoor and Mobile Radio Communications, pp. 1-5. , IEEE, Helsinki, Finland; Pilu, M., Extraction of illusory linear clues in perspectively skewed documents (2001) Proc. of IEEE Conf. Computer Vision and Pattern Recognition, pp. I363-I368. , IEEE, Kauai; Monnier, C., Sequential correction of perspective warp in camerabased documents (2005) Proc. of Int. Conf. Document Analysis and Recognition, pp. 394-398. , IEEE, Seoul, Korea; Clark, P., Mirmehdi, M., Rectifying perspective views of text in 3D scenes using vanishing points (2003) Pattern Recognit., 36 (11), pp. 2673-2686; Yin, X.C., Robust vanishing point detection for mobilecam-based documents (2011) Proc. of Int. Conf. Document Analysis and Recognition, pp. 136-140. , IEEE, Beijing, China; Meng, G., Metric rectification of curved document images (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (4), pp. 707-722; Muammar, H., Dragotti, P., An investigation into aliasing in images recaptured from an LCD monitor using a digital camera (2013) Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, pp. 2242-2246. , IEEE, Vancouver, Canada; Tian, Y., Narasimhan, S.G., Rectification and 3D reconstruction of curved document images (2011) Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 377-384. , IEEE, Colorado Springs; Zhang, L., Zhang, Y., Tan, C.L., An improved physically-based method for geometric restoration of distorted document images (2008) IEEE Trans. Pattern Anal. Mach. Intell., 30 (4), pp. 728-734; Tan, C.L., Xhang, L., Xia, T., Restoring warped document images through 3-D shape modeling (2006) IEEE Trans. Pattern Anal. Mach. Intell., 28 (2), pp. 195-208; Liang, J., Dementhon, D., Doermann, D., Geometric rectication of camera-captured document images (2008) IEEE Trans. Pattern Anal. Mach. Intell., 30 (4), pp. 728-734; Cao, Y., Wang, S., Li, H., Skew detection and correction in document images based on straight-line fitting (2003) Pattern Recognit., 24 (12), pp. 1871-1879; Kwag, H.K., Efficient skew estimation and correction algorithm for document images (2002) Image Vision Comput., 20 (1), pp. 25-35; Brown, M.S., Tsoi, Y.C., Geometric and shading correction for images of printed materials using boundary (2006) IEEE Trans. Image Process., 15 (6), pp. 1544-1554; Brown, M.S., Restoring 2D content from distorted documents (2007) IEEE Trans. Pattern Anal. Mach. Intell., 29 (11), pp. 1094-1916; Tsoi, Y.C., Brown, M.S., Multi-view document rectification using boundary (2007) Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 1-8. , IEEE, Minneapolis, Minnesota; Banerjee, S., Real-time embedded skew detection and frame removal (2010) Proc. IEEE Int. Conf. Image Processing, pp. 2181-2184. , IEEE, Hong Kong; Lu, S., Chen, B.M., Ko, C.C., Perspective rectification of document images using fuzzy set and morphological operations (2005) Image Vision Comput., 23 (5), pp. 541-553; Lu, S., Chen, B.M., Ko, C.C., A partition approach for the restoration of camera images of plannar and curled document (2006) Image Vision Comput., 24 (8), pp. 837-848; Saveljev, V.V., About a moiré-less condition for non-square grids (2008) J. Disp. Technol., 4 (3), pp. 332-339; Han, Y., Kim, J., Kim, M., A new moiré smoothing method for color inverse halftoning (2002) Proc. IEEE Int. Conf. Image Processing, pp. I820-I823. , IEEE, Rochester; Siddiqui, H., Bouman, C.A., Training-based algorithm for moiré suppression in scanned halftone images (2007) Proc. of SPIE, 6498, p. 64981D; Ville, D.V.D., Suppression of sampling moiré in color printing by spline-based least-squares prefiltering (2003) Pattern Recognit., 24 (11), pp. 1787-1794; Cao, H., Kot, A.C., Identification of recaptured photographs on LCD screens (2010) Proc. IEEE Int. Conf. Acoustics Speech and Signal Processing, pp. 1790-1793. , IEEE, Dallas; Pilu, M., Pollard, S., A light-weight text image processing method for handheld embedded cameras (2002) Proc. British Machine Computer Vision, pp. 547-556. , BMVA Press, Cardiff, UK; Liu, Y., Srihari, S., Document image binarization based on texture features (1997) IEEE Trans. Pattern Anal. Mach. Intell., 19 (5), pp. 540-544; Pei, S.C., Tzeng, M., Hsiao, Y., Enhancement of uneven lighting text image using linebased empirical mode decomposition (2011) Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, pp. 1249-1252. , IEEE, Prague, Czech Republic; Kuk, J.G., Cho, N.I., Feature based binarization of document images degraded by uneven light condition (2009) Proc. Int. Conf. Document Analysis and Recognition, pp. 748-752. , IEEE, Barcelona, Spain; Tabataei, S.A., Bohlool, M., A novel method for binarization of badly illuminated document images (2010) Proc. Int. Conf. Image Processing, pp. 3573-3576. , IEEE, Hong Kong; Shi, Z., Govindaraju, V., Historical document image enhancement using background light intensity normalization (2004) Proc. Int. Conf. Pattern Recognition, pp. 473-476. , IEEE, Cambridge, UK; Sharma, A., Mahaldar, S., Banerjee, S., Enhanced bleed through removal for scanned document images (2011) IS&T/SPIE Electronic Imaging, p. 787019. , SPIE, San Francisco; Bradley, D., Roth, G., Adaptive thresholding using the integral image (2007) J. Graph. GPU Game Tools, 12 (2), pp. 13-21; Clark, P., Mirmehdi, M., Finding text regions using localised statistical measures (2000) Proc. of British Machine Vision Conf., pp. 675-684. , BMVA Press, Bristol, UK; Duda, R.O., Hart, P.E., Use of the hough transformation to detect lines and curves in pictures (1972) Commun. ACM, 15 (1), pp. 11-15; Tomasi, C., Manduchi, R., Bilateral filtering for gray and color images (1998) Proc. IEEE Int. Conf. Computer Vision, pp. 839-846. , IEEE, Bombay, India","Park, I.K.; Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, South Korea",,,"SPIE",,,,,10179909,,JEIME,,"English","J. Electron. Imaging",Article,"Final","All Open Access, Green",Scopus,2-s2.0-84924158810
"Williem, Tai Y.-W., Park I.K.","51261491100;7201915847;8612277600;","Accurate and real-time depth video acquisition using Kinect-stereo camera fusion",2014,"Optical Engineering","53","4", 043110,"","",,4,"10.1117/1.OE.53.4.043110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901049561&doi=10.1117%2f1.OE.53.4.043110&partnerID=40&md5=de4770432f86a24998bfceee646dc6a8","Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon 402-751, South Korea; Korea Advanced Institute of Science and Technology, Department of Electrical Engineering, 291 Daehak-ro, Yuseong-gu, Daejeon 305-701, South Korea","Williem, Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon 402-751, South Korea; Tai, Y.-W., Korea Advanced Institute of Science and Technology, Department of Electrical Engineering, 291 Daehak-ro, Yuseong-gu, Daejeon 305-701, South Korea; Park, I.K., Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon 402-751, South Korea","This paper presents a Kinect-stereo camera fusion system that significantly improves the accuracy of depth map acquisition. The typical Kinect depth map suffers from missing depth values and errors, resulting from a single Kinect input. To ameliorate such problems, the proposed system couples a Kinect with a stereo RGB camera to provide an additional disparity map. Kinect depth map and the disparity map are efficiently fused in real time by exploiting a spatiotemporal Markov random field framework on a graphics processing unit. An efficient temporal data cost is proposed to maintain the temporal coherency between frames. We demonstrate the performance of the proposed system on challenging real-world examples. Experimental results confirm that the proposed system is robust and accurate in depth video acquisition. © 2014 Society of Photo-Optical Instrumentation Engineers.","depth correspondence; Kinect-stereo fusion; real-time stereo matching; stereo matching; three-dimensional computer vision","Cameras; Computer graphics; Markov processes; Program processors; depth correspondence; Fusion systems; Graphics Processing Unit; Markov Random Fields; Real time stereo; Stereo matching; Temporal coherency; Three-dimensional computer vision; Stereo image processing",,,,"NRF-2012R1A1A2009495

National Research Foundation of Korea, NRF","This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2012R1A1A2009495).",,"Zhu, J., Spatial-temporal fusion for high accuracy depth maps using dynamic MRFs (2010) IEEE Trans. Pattern Anal. Mach. Intell., 32 (5), pp. 899-909; Yang, Q., Real-time global stereo matching using hierarchical belief propagation (2006) Proc. of British Machine Vision Conf., pp. 989-998. , British Machine Vision AsSoCiation, Edinburgh, UK; Hosni, A., Real-time local stereo matching using guided image filtering (2011) Proc. of IEEE Int. Conf. on Multimedia & Expo, pp. 1-6. , IEEE, Barcelona, Spain; Richardt, C., Real-time spatIoTemporal stereo matching using the dual-cross-bilateral grid (2010) Prof. of European Conf. on Computer Vision, pp. 510-523. , Springer, Heraklion, Crete, Greece; Kowalczuk, J., Psota, E.T., Perez, L.C., Real-time stereo matching on CUDA using an iterative refinement method for adaptive supportweight correspondences (2013) IEEE Trans. Circuits Syst. Video Technol., 23 (1), pp. 94-104; De-Maeztu, L., Villanueva, A., Cabeza, R., Near real-time stereo matching using geode SiC diffusion (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (2), pp. 410-416; Yang, Q., Engels, C., Akbarzadeh, A., Near real-time stereo for weakly-textured scenes (2008) Proc. of British Machine Vision Conf., pp. 1-10. , British Machine Vision AsSoCiation, Leeds, UK; Zhang, K., Real-time accurate stereo with bitwise fast voting on CUDA (2009) Proc. of IEEE Int. Conf. on Computer Vision Workshops, pp. 794-800. , IEEE, Kyoto, Japan; Yang, Q., Spatial-depth super resolution for range images (2007) Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, pp. 1-8. , IEEE, Minneapolis, Minnesota; Dolson, J., Upsampling range data in dynamic environments (2010) Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, pp. 1141-1148. , IEEE, San Francisco, California; Park, J., High quality depth map upsampling for 3D-ToF cameras (2011) Proc. of IEEE Int. Conf. on Computer Vision, pp. 1623-1630. , IEEE, Barcelona, Spain; Zhu, J., Reliability fusion of time-of-flight depth and stereo geometry for high quality depth maps (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33 (7), pp. 1400-1414; Nair, R., High accuracy ToF and stereo sensor fusion at interactive rates (2012) Proc. of European Conf. on Computer Vision Workshops, pp. 1-11. , Springer, Florence, Italy; Mutto, C.D., Locally consistent ToF and stereo data fusion (2012) Proc. of European Conf. on Computer Vision Workshops, pp. 598-607. , Springer, Florence, Italy; Wang, Y., Jia, Y., A fusion framework of stereo vision and kinect for high-quality dense depth maps (2012) Proc. of Asian Conf. on Computer Vision Workshops, pp. 109-120. , Springer, Daejeon, Korea; Chiu, W.C., Blanke, U., Fritz, M., Improving the kinect by crossmodal stereo (2011) Proc. of British Machine Vision Conf., pp. 1-10. , British Machine Vision AsSoCiation, Dundee, UK; Larsen, E.S., Temporally consistent reconstruction from multiple video streams using enhanced belief propagation (2007) Proc. of IEEE Int. Conf. on Computer Vision, pp. 1-8. , IEEE, Rio de Janeiro, Brazil; Yao, L., Li, D.X., Zhang, M., Temporally consistent depth maps recovery from stereo vision (2012) Inf. Technol. J., 11 (1), pp. 30-39; Scharstein, D., Szeliski, R., A taxonomy and evaluation of dense two-frame stereo correspondence algorithms (2002) International Journal of Computer Vision, 47 (1-3), pp. 7-42. , DOI 10.1023/A:1014573219977; Park, I.K., Design and performance evaluation of image processing algorithms on GPUs (2011) IEEE Trans. Parallel Distrib. Syst., 22 (1), pp. 91-104; Yu, L.-F., Shading-based shape refinement of RGB-D images (2013) Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, pp. 1415-1422. , IEEE, Portland, Oregon; Bruhn, A., Real-time optic flow computation with variational methods (2003) Proc. of Computer Analysis of Images and Patterns, pp. 222-229. , Springer, Münster, Germany; Hermann, S., Vaudrey, T., The gradient-A powerful and robust cost function for stereo matching (2010) Proc. of Int. Conf. of Image and Vision Computing New Zealand, pp. 1-8. , IEEE, Queenstown, New Zealand; Felzenszwalb, P.F., Huttenlocher, D.P., Efficient belief propagation for early vision (2006) International Journal of Computer Vision, 70 (1), pp. 41-54. , DOI 10.1007/s11263-006-7899-4; Tappen, M., Freeman, W., Comparison of graph cuts with belief propagation for stereo, using identical MRF parameters (2003) Proc. of IEEE Int. Conf. on Computer Vision, pp. 900-906. , IEEE, Nice, France","Park, I.K.; Inha University, Department of Information and Communication Engineering, 100 Inha-ro, Nam-gu, Incheon 402-751, South Korea; email: pik@inha.ac.kr",,,"SPIE",,,,,00913286,,OPEGA,,"English","Opt Eng",Article,"Final","",Scopus,2-s2.0-84901049561
"Williem I., Simon C., Cho S., Park I.K.","51261491100;56179982100;36061583100;8612277600;","Fast and robust perspective rectification of document images on a smartphone",2014,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,, 6909982,"197","198",,4,"10.1109/CVPRW.2014.37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908508110&doi=10.1109%2fCVPRW.2014.37&partnerID=40&md5=6d502908ab2642c0834fcee938a25d62","Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea; Mobile Communication Division, Samsung Electronics Co. Ltd., Suwon, 443-742, South Korea","Williem, I., Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea; Simon, C., Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea; Cho, S., Mobile Communication Division, Samsung Electronics Co. Ltd., Suwon, 443-742, South Korea; Park, I.K., Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea","This paper presents a perspective rectification framework for mobile device that is fast and robust to recovering the fronto-parallel view of perspectively distorted document images. The conventional approaches is too heavy to be implemented on a mobile device. In addition, they fails to reject the false case of the perspective rectification. To ameliorate such problems, the proposed framework is designed to pursue a fast and robust algorithm to detect horizontal and vertical vanishing points efficiently and robustly. Then, perspective rectification is performed using both horizontal and vertical vanishing points. In addition, the proposed framework has an adaptive scheme to detect the false case of the perspective rectification and skip the procedure without using the vertical vanishing point. Note that, the proposed framework is designed for consumer application so that bad results are rejected before they are shown for users. We demonstrate the performance of the proposed framework on various challenging examples to confirm that the proposed system is fast and robust in rectifying the perspectively distorted images. © 2014 IEEE.",,"Computer vision; Mobile devices; Algorithms; Computer vision; Mobile devices; Adaptive scheme; Consumer applications; Conventional approach; Distorted images; Document images; Perspective rectifications; Robust algorithm; Vanishing point; Algorithms; Pattern recognition",,,,,,,"Clark, P., Mirmehdi, M., Rectifying perspective views of text in 3D scenes using vanishing point (2003) Pattern Recognition, 36 (11), pp. 2673-2686. , November; Duda, R.O., Hart, P.E., Use of the hough transformation to detect lines and curves in picture (1972) Communications of the ACM, 15 (1), pp. 11-15. , January; Yin, X.C., Hao, H.W., Sun, J., Naoi, S., Robust vanishing point detection for mobilecam-based document (2011) Proc. of International Conference on Document Analysis and Recognition, pp. 136-140. , September","Williem, I.; Dept. of Information and Communication Eng., Inha UniversitySouth Korea",,"","IEEE Computer Society","2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW 2014","23 June 2014 through 28 June 2014",,114805,21607508,9781479943098; 9781479943098,,,"English","IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops",Conference Paper,"Final","",Scopus,2-s2.0-84908508110
"Seiller N., Williem, Singhal N., Park I.K.","24830852200;51261491100;24480138200;8612277600;","Object oriented framework for real-time image processing on GPU",2014,"Multimedia Tools and Applications","70","3",,"2347","2368",,2,"10.1007/s11042-013-1440-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905594245&doi=10.1007%2fs11042-013-1440-x&partnerID=40&md5=3bd68086efbbe98027d3132af186b0fb","SmartCo, Paris 75008, France; Biomedical Signal Analysis Lab, GE Global Research, Bangalore 560066, India; School of Information and Communication Engineering, Inha University, Incheon 402-751, South Korea","Seiller, N., SmartCo, Paris 75008, France; Williem, School of Information and Communication Engineering, Inha University, Incheon 402-751, South Korea; Singhal, N., Biomedical Signal Analysis Lab, GE Global Research, Bangalore 560066, India; Park, I.K., School of Information and Communication Engineering, Inha University, Incheon 402-751, South Korea","General purpose computation on graphics processing unit (GPGPU) provides a significant gain in terms of the processing time compared with CPU. Images are particularly good subjects for massive parallel implementations on GPU. Thus, the processing time can be improved for computer vision and image/video processing algorithms. However, GPGPU has a fairly complex integration process in a framework and they evolve very rapidly. In this paper, we present a framework that provides all the desired primitives related to GPGPU-based image processing algorithms, which makes it easy and straightforward for the user to exploit. The proposed framework is object-oriented, and it utilizes design patterns. The user can benefit from all the advantages of object-oriented programming, such as code reusability/extensibility, flexibility, information hiding, and complexity hiding. This makes it possible to rapidly integrate new technologies and functionality as they appear. © Springer Science+Business Media New York 2013.","Computer vision; Design patterns; GPGPU; Image/video processing; Object-oriented","Computer graphics; Computer vision; Image processing; Object oriented programming; Program processors; Video cameras; Design Patterns; General Purpose Computation on Graphics Processing Unit (GPGPU); GPGPU; Image processing algorithm; Object oriented; Object-oriented frameworks; Parallel implementations; Real-time image processing; Computer graphics equipment",,,,"Ministry of Knowledge Economy, MKE","Acknowledgements This work was supported by the Industrial Strategic Technology Development Program (10041664, The Development of Fusion Processor based on Multi-Shader GPU) funded by the Ministry of Knowledge Economy (MKE, Korea).",,"Allusse, Y., Horain, P., Agarwal, A., Saipriyadarshan, C., GpuCV: An opensource GPUaccelerated framework for image processing and computer vision (2008) Proc. of the 16th ACM International Conference on Multimedia, pp. 1089-1092; Babenko, P., Shah, M., MinGPU: A minimum GPU library for computer vision (2008) Real-Time Image Process, 3 (4), pp. 255-268; Bradski, G., Kaehler, A., (2008) Learning OpenCV: Computer Vision with the OpenCV Library, , O'Reilly; http://www.caps-entreprise.com/index.php, CAPS Enterprise: HMMP Workbench; Chang, J.Y., Park, H., Park, I.K., Lee, K.M., Lee, S.U., Gpu-friendly multi-view stereo reconstruction using surfel representation and graph cuts (2011) Comput Vis Image Underst, 115 (5), pp. 620-634; Fung, J., Mann, S., Aimone, C., OpenVIDIA: Parallel GPU computer vision (2005) Proc. of the 13th Annual ACM International Conference on Multimedia, pp. 849-852; Gamma, E., Helm, R., Johnson, R., Vlissides, J., (1994) Design Patterns: Elements of Reusable Objectoriented Software, , Addison-Wesley, Reading; http://www.gpgpu.org, General Purpose GPU Programming (GPGPU); Hou, Q., Zhou, K., Guo, B., BSGP: Bulk-synchornous GPU programming (2008) ACMTrans Graph, 27 (3), pp. 1-12; Jansen, T., (2007) GPU++, An Embedded GPU Development System for General-purpose Computations, , Ph.D. Thesis, Technical University Munich; http://www.khronos.org/opencl/, Khronos Group: Open computing language; Kirk, D., Hwu, W., (2010) Programming Massively Parallel Processors: A Hands-on Approach, , Morgan Kaufmann, San Mateo; Kuck, R., Wesche, G., A framework for object-oriented shader design (2009) Proc. Intl. Symposium on Advances in Visual Computing, pp. 1019-1030; McCool, M., Toit, S.D., Popa, T., Chan, B., Moule, K., Shader algebra (2004) ACM Trans Graph, 23 (3), pp. 784-792; Membarth, R., Lokhmotov, A., Teich, J., Generating GPU code from a high-level representation for image processing kernels (2011) HPPC 2011, p. 28; Nevatia, R., Babu, K.R., Linear feature extraction and description (1980) Comput Graph Image Process, 13 (3), pp. 257-269; Nguyen, V., Deeds-Rubin, S., Tan, T., Boehm, B., A SLOC coding standard (2007) Proc. International Annual Forum on COCOMO and Systems/software Cost Modeling, pp. 1-16; http://www.nvidia.com/object/npp.html; http://http://opencv.willowgarage.com/wiki/OpenCV_GPU; Owens, J.D., Houston, M., Luebke, D., Green, S., Stone, J.E., Phillips, J.C., GPU computing (2008) Proc IEEE, 96 (5), pp. 879-899; Park, I.K., Singhal, N., Lee, M.H., Cho, S., Kim, C.W., Design and performance evaluation of image processing algorithms on GPUs (2011) IEEE Trans Parallel Distrib Syst, 22 (1), pp. 91-104; Raspe, M., (2009) GPU-assisted Diagnosis and Visualization of Medical Volume Data, , Ph.D. Thesis, University of Koblenz and Landau; Rost, R., (2006) OpenGL Shading Language, , Addison-Wesley, Reading; http://www.pgroup.com/resources/accel.htm, The Portland Group: PGI accelerator compilers","Park, I.K.; School of Information and Communication Engineering, Inha University, Incheon 402-751, South Korea; email: pik@inha.ac.kr",,,"Kluwer Academic Publishers",,,,,13807501,,MTAPF,,"English","Multimedia Tools Appl",Article,"Final","All Open Access, Green",Scopus,2-s2.0-84905594245
"Simon C., Williem I., Choe J., Yun I.D., Park I.K.","56179982100;51261491100;57220658721;55666861500;8612277600;","Correcting photometric distortion of document images on a smartphone",2014,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,, 6909983,"199","200",,1,"10.1109/CVPRW.2014.38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908555263&doi=10.1109%2fCVPRW.2014.38&partnerID=40&md5=7fbd386601a67d30f92604438d2b747b","Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea; Mobile Communication Division, Samsung Electronics Co. Ltd., Suwon, 443-742, South Korea; Dept. of Digital Information Eng., Hankuk University of Foreign Studies, Yongin, 449-791, South Korea","Simon, C., Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea; Williem, I., Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea; Choe, J., Mobile Communication Division, Samsung Electronics Co. Ltd., Suwon, 443-742, South Korea; Yun, I.D., Dept. of Digital Information Eng., Hankuk University of Foreign Studies, Yongin, 449-791, South Korea; Park, I.K., Dept. of Information and Communication Eng., Inha University, Incheon, 402-751, South Korea","This paper presents efficient and robust methods for correcting photometric distortion on document images caused by moir'e pattern noise and specular highlight in smartphone. Our algorithm uses separated smoothing process for moir'e pattern removal on recaptured document image from LCD monitor. Furthermore, contrast of characters in specular highlight area is enhanced using subtraction and noise removal technique. © 2014 IEEE.",,"Computer vision; Liquid crystal displays; Signal encoding; Smartphones; Computer vision; Liquid crystal displays; Photometry; Signal encoding; Smartphones; Document images; LCD monitors; Noise removal; Pattern noise; Photometric distortions; Robust methods; Smoothing process; Specular highlight; Photometry; Pattern recognition",,,,,,,"Bradley, D., Roth, G., Adaptive thresholding using the integral imag (2007) Journal of Graphics, GPU, and Game Tools, 12 (2), pp. 13-21. , January; Liu, Y., Srihari, S., Document image binarization based on texture feature (1997) IEEE Trans. on Pattern Analysis and Machine Intelligence, 19 (5), pp. 540-544. , May","Simon, C.; Dept. of Information and Communication Eng., Inha UniversitySouth Korea",,"","IEEE Computer Society","2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW 2014","23 June 2014 through 28 June 2014",,114805,21607508,9781479943098; 9781479943098,,,"English","IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84908555263
"Choi Y.-K., Williem, Park I.K.","56896460100;51261491100;8612277600;","Memory-efficient belief propagation in stereo matching on GPU",2012,"2012 Conference Handbook - Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2012",,, 6411871,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874427019&partnerID=40&md5=ceefec8d787b2a9a6013c055285c29e2","Inha University, Incheon 402-751, South Korea","Choi, Y.-K., Inha University, Incheon 402-751, South Korea; Williem, Inha University, Incheon 402-751, South Korea; Park, I.K., Inha University, Incheon 402-751, South Korea","Belief propagation (BP) is a commonly used global energy minimization algorithm for solving stereo matching problem in 3D reconstruction. However, it requires large memory bandwidth and data size. In this paper, we propose a novel memory-efficient algorithm of BP in stereo matching on the Graphics Processing Units (GPU). The data size and transfer bandwidth are significantly reduced by storing only a part of the whole message. In order to maintain the accuracy of the matching result, the local messages are reconstructed using shared memory available in GPU. Experimental result shows that there is almost an order of reduction in the global memory consumption, and 21 to 46% saving in memory bandwidth when compared to the conventional algorithm. The implementation result on a recent GPU shows that we can obtain 22.8 times speedup in execution time compared to the execution on CPU. © 2012 APSIPA.",,"3D reconstruction; Belief propagation; Conventional algorithms; Data size; Execution time; Global energy; Graphics Processing Unit; Memory bandwidths; Memory consumption; Memory-efficient algorithms; Shared memories; Stereo matching; Algorithms; Computer graphics; Data processing; Program processors; Backpropagation",,,,,,,"Choi, Y., CUDA implementation of belief propagation for stereo vision (2010) Proc. IEEE Int. Conf. Intelligent Transportation Systems, pp. 1402-1407. , Sept; Felzenszwalb, P.F., Huttenlocher, D.P., Efficient belief propagation for early vision (2006) International Journal of Computer Vision, 70 (1), pp. 41-54. , DOI 10.1007/s11263-006-7899-4; Liang, C.-K., Cheng, C.-C., Lai, Y.-C., Chen, L.-G., Chen, H.H., Hardware-efficient belief propagation (2011) IEEE Trans. Circuits and Systems for Video Technology, 21 (5), pp. 525-537. , May; Scharstein, D., Szeliski, R., A taxonomy and evaluation of dense two-frame stereo correspondence algorithms (2002) International Journal of Computer Vision, 47 (1-3), pp. 7-42. , DOI 10.1023/A:1014573219977; Sun, J., Shum, H., Zheng, N., Stereo matching using belief propagation (2003) IEEE Trans. Pattern Analysis and Machine Intelligence, 25 (7), pp. 787-800. , July; Tappen, M., Freeman, W., Comparison of graph cuts with belief propagation for stereo, using identical MRF parameters (2003) Proc. IEEE Int. Conf. Computer Vision, 2, pp. 900-906. , Oct; Yang, Q., Wang, L., Ahuja, N., A constant-space belief propagation algorithm for stereo matching (2010) Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 1458-1465. , June; Yu, T., Lin, R.-S., Super, B., Tang, B., Eficient message representations for belief propagation (2007) Proc. IEEE Int. Conf. Computer Vision, pp. 1-8. , Oct","Choi, Y.-K.; Inha University, Incheon 402-751, South Korea; email: ykchoi@cs.ucla.edu",,,,"2012 4th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2012","3 December 2012 through 6 December 2012","Hollywood, CA",95651,,9780615700502,,,"English","Conf. Handb. - Asia-Pac. Signal Inf. Process. Assoc. Annu. Summit Conf., APSIP",Conference Paper,"Final","",Scopus,2-s2.0-84874427019
